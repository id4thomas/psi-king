{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docling VLM Test\n",
    "* docling v2.25.0 release introduces VLM pipeline\n",
    "    * https://github.com/DS4SD/docling/releases/tag/v2.25.0\n",
    "* example: [[link]](https://github.com/DS4SD/docling/blob/37dd8c1cc7fc05095fe889eec78300647c946a42/docs/examples/minimal_vlm_pipeline.py#L10)\n",
    "\n",
    "## VlmPipeline\n",
    "* https://github.com/DS4SD/docling/blob/37dd8c1cc7fc05095fe889eec78300647c946a42/docling/pipeline/vlm_pipeline.py#L47\n",
    "\n",
    "\n",
    "## VlmPipelineOptions\n",
    "* https://github.com/DS4SD/docling/blob/37dd8c1cc7fc05095fe889eec78300647c946a42/docling/datamodel/pipeline_options.py#L334\n",
    "\n",
    "```\n",
    "class VlmPipelineOptions(PaginatedPipelineOptions):\n",
    "    artifacts_path: Optional[Union[Path, str]] = None\n",
    "\n",
    "    generate_page_images: bool = True\n",
    "    force_backend_text: bool = (\n",
    "        False  # (To be used with vlms, or other generative models)\n",
    "    )\n",
    "    # If True, text from backend will be used instead of generated text\n",
    "    vlm_options: Union[HuggingFaceVlmOptions] = smoldocling_vlm_conversion_options\n",
    "```\n",
    "\n",
    "## VlmOptions\n",
    "* uses response_format `DOCTAGS`\n",
    "\n",
    "```\n",
    "class HuggingFaceVlmOptions(BaseVlmOptions):\n",
    "    kind: Literal[\"hf_model_options\"] = \"hf_model_options\"\n",
    "\n",
    "    repo_id: str\n",
    "    load_in_8bit: bool = True\n",
    "    llm_int8_threshold: float = 6.0\n",
    "    quantized: bool = False\n",
    "\n",
    "    response_format: ResponseFormat\n",
    "\n",
    "    @property\n",
    "    def repo_cache_folder(self) -> str:\n",
    "        return self.repo_id.replace(\"/\", \"--\")\n",
    "\n",
    "\n",
    "smoldocling_vlm_conversion_options = HuggingFaceVlmOptions(\n",
    "    repo_id=\"ds4sd/SmolDocling-256M-preview\",\n",
    "    prompt=\"Convert this page to docling.\",\n",
    "    response_format=ResponseFormat.DOCTAGS,\n",
    ")\n",
    "\n",
    "granite_vision_vlm_conversion_options = HuggingFaceVlmOptions(\n",
    "    repo_id=\"ibm-granite/granite-vision-3.1-2b-preview\",\n",
    "    # prompt=\"OCR the full page to markdown.\",\n",
    "    prompt=\"OCR this image.\",\n",
    "    response_format=ResponseFormat.MARKDOWN,\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "doctags example:\n",
    "* html like format (similar to qwen2-vl html)\n",
    "```\n",
    "<document>\n",
    "<subtitle-level-1><location><page_1><loc_18><loc_85><loc_83><loc_89></location>DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis</subtitle-level-1>\n",
    "<paragraph><location><page_1><loc_15><loc_77><loc_32><loc_83></location>Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com</paragraph>...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    AcceleratorDevice,\n",
    "    VlmPipelineOptions,\n",
    "    HuggingFaceVlmOptions,\n",
    "    ResponseFormat\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "\n",
    "from config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\n",
    "    # \"tests/data/2305.03393v1-pg9-img.png\",\n",
    "    \"tests/data/1706.03762v7_pg1_3.pdf\",\n",
    "]\n",
    "\n",
    "## Use experimental VlmPipeline\n",
    "pipeline_options = VlmPipelineOptions()\n",
    "# If force_backend_text = True, text from backend will be used instead of generated text\n",
    "pipeline_options.force_backend_text = False\n",
    "pipeline_options.generate_picture_images = True\n",
    "\n",
    "pipeline_options.accelerator_options.device = AcceleratorDevice.MPS\n",
    "\n",
    "model_dir = os.path.join(\n",
    "    settings.docling_model_weight_dir, \"granite-vision-3.1-2b-preview\"\n",
    ")\n",
    "\n",
    "pipeline_options.artifacts_path=settings.docling_model_weight_dir\n",
    "vlm_conversion_options = HuggingFaceVlmOptions(\n",
    "    repo_id = \"granite-vision-3.1-2b-preview\",\n",
    "    # repo_id=model_dir,\n",
    "    prompt=\"OCR the full page to markdown.\",\n",
    "    # prompt=\"OCR this image.\",\n",
    "    response_format=ResponseFormat.MARKDOWN,\n",
    "    # response_format=ResponseFormat.DOCTAGS,\n",
    "    load_in_8bit=False,\n",
    "    quantized=False\n",
    ")\n",
    "\n",
    "## Pick a VLM model. We choose SmolDocling-256M by default\n",
    "pipeline_options.vlm_options = vlm_conversion_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_cls=VlmPipeline,\n",
    "            pipeline_options=pipeline_options,\n",
    "        ),\n",
    "        InputFormat.IMAGE: PdfFormatOption(\n",
    "            pipeline_cls=VlmPipeline,\n",
    "            pipeline_options=pipeline_options,\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/docling/lib/python3.10/site-packages/transformers/pytorch_utils.py:338: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    }
   ],
   "source": [
    "# source = \"../samples/1706.03762v7_p3.png\"\n",
    "source = \"../samples/2305.03393v1-pg9-img.png\" # used in docling example\n",
    "source = \"../samples/1706.03762v7_pg1_3.pdf\" # 3 pages of attention is all you need\n",
    "\n",
    "res = converter.convert(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n",
      "Predicted page in DOCTAGS:\n",
      "|    | 0              | 1              | 2              | 3              |\n",
      "|---:|:---------------|:---------------|:---------------|:---------------|\n",
      "|  0 | Ashish Vaswani* | Noam Shazeer*   | Niki Parmar*   | Jakob Uszkoreit* |\n",
      "|  1 | Google Brain   | Google Brain   | Google Research | Google Research |\n",
      "|  2 | awvansing@google.com | tooming@google.com | nikip@google.com | nikip@google.com |\n",
      "|  3 | Ilion Jones*   | Aidan N. Gomez* | Lukasz Kaiser* | Lukasz Kaiser* |\n",
      "|  4 | Google Research | University of Toronto | Google Brain   | Google Brain   |\n",
      "|  5 | 11ion@google.com | aidan@cs.toronto.edu | lukaszkaiser@google.com | lukaszkaiser@google.com |\n",
      "|  6 | Illia Polosukhin* | Illia Polosukhin* | Illia Polosukhin* | Illia Polosukhin* |<|end_of_text|>\n",
      "\n",
      "Predicted page in DOCTAGS:\n",
      "\n",
      "|    | 0                                                                                                                                                                                                   |\n",
      "|---:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 | 1 Introduction                                                                                                                                                                                                 |\n",
      "|  1 | Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [25, 21, 5]. |\n",
      "|  2 | Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [3, 12, 24, 15].                                                                                                          |\n",
      "|  3 | Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t, as a function of the previous hidden state h t -1 and the input for position t. |\n",
      "|  4 | This inherently sequential nature precludes parallelization within training examples, which become critical as longer sequence lengths, as memory constraints limit batching across examples.                                                                                  |\n",
      "|  5 | Recent works have achieved significant improvements to computational efficiency through factorization tricks [22] and computational computation [31] while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. |\n",
      "|  6 | Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [21, 19]. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network. |\n",
      "|  7 | In this work we propose the Transformer, a model architecture eschewing recurrent and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. |<|end_of_text|>\n",
      "\n",
      "Predicted page in DOCTAGS:\n",
      "\n",
      "|    | 0                                                                     |\n",
      "|---:|:----------------------------------------------------------------------|\n",
      "|  0 | Encoder: The encoder is composed of a stack of N = 6 identical layers. |\n",
      "|  1 | Each layer has two sub-layers. The first is a multi-head self-attention |\n",
      "|  2 | mechanism, and the second is a simple, position-wise fully connected network. |<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(len(res.pages))\n",
    "for page in res.pages:\n",
    "    print(\"\")\n",
    "    print(\"Predicted page in DOCTAGS:\")\n",
    "    print(page.predictions.vlm_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | 0                    | 1                     | 2                       | 3                       |\n",
      "|----|----------------------|-----------------------|-------------------------|-------------------------|\n",
      "|  0 | Ashish Vaswani*      | Noam Shazeer*         | Niki Parmar*            | Jakob Uszkoreit*        |\n",
      "|  1 | Google Brain         | Google Brain          | Google Research         | Google Research         |\n",
      "|  2 | awvansing@google.com | tooming@google.com    | nikip@google.com        | nikip@google.com        |\n",
      "|  3 | Ilion Jones*         | Aidan N. Gomez*       | Lukasz Kaiser*          | Lukasz Kaiser*          |\n",
      "|  4 | Google Research      | University of Toronto | Google Brain            | Google Brain            |\n",
      "|  5 | 11ion@google.com     | aidan@cs.toronto.edu  | lukaszkaiser@google.com | lukaszkaiser@google.com |\n",
      "|  6 | Illia Polosukhin*    | Illia Polosukhin*     | Illia Polosukhin*       | Illia Polosukhin*       |\n",
      "\n",
      "|    | 0                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|----|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 | 1 Introduction                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|  1 | Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [25, 21, 5].                                                                                                        |\n",
      "|  2 | Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [3, 12, 24, 15].                                                                                                                                                                                                                                                   |\n",
      "|  3 | Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t, as a function of the previous hidden state h t -1 and the input for position t.                                                                                                |\n",
      "|  4 | This inherently sequential nature precludes parallelization within training examples, which become critical as longer sequence lengths, as memory constraints limit batching across examples.                                                                                                                                                                                                  |\n",
      "|  5 | Recent works have achieved significant improvements to computational efficiency through factorization tricks [22] and computational computation [31] while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.                                                                                                     |\n",
      "|  6 | Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [21, 19]. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.                                         |\n",
      "|  7 | In this work we propose the Transformer, a model architecture eschewing recurrent and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. |\n",
      "\n",
      "|    | 0                                                                             |\n",
      "|----|-------------------------------------------------------------------------------|\n",
      "|  0 | Encoder: The encoder is composed of a stack of N = 6 identical layers.        |\n",
      "|  1 | Each layer has two sub-layers. The first is a multi-head self-attention       |\n",
      "|  2 | mechanism, and the second is a simple, position-wise fully connected network. |\n"
     ]
    }
   ],
   "source": [
    "print(res.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoclingDocument(schema_name='DoclingDocument', version='1.1.0', name='1706.03762v7_pg1_3', origin=DocumentOrigin(mimetype='text/markdown', binary_hash=6974430905591189644, filename='1706.03762v7_pg1_3.pdf', uri=None), furniture=GroupItem(self_ref='#/furniture', parent=None, children=[], content_layer=<ContentLayer.FURNITURE: 'furniture'>, name='_root_', label=<GroupLabel.UNSPECIFIED: 'unspecified'>), body=GroupItem(self_ref='#/body', parent=None, children=[RefItem(cref='#/tables/0'), RefItem(cref='#/tables/1'), RefItem(cref='#/tables/2')], content_layer=<ContentLayer.BODY: 'body'>, name='_root_', label=<GroupLabel.UNSPECIFIED: 'unspecified'>), groups=[], texts=[], pictures=[], tables=[TableItem(self_ref='#/tables/0', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TABLE: 'table'>, prov=[], captions=[], references=[], footnotes=[], image=None, data=TableData(table_cells=[TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=0, end_col_offset_idx=1, text='', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=1, end_col_offset_idx=2, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=2, end_col_offset_idx=3, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=3, end_col_offset_idx=4, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=4, end_col_offset_idx=5, text='3', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=0, end_col_offset_idx=1, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=1, end_col_offset_idx=2, text='Ashish Vaswani*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=2, end_col_offset_idx=3, text='Noam Shazeer*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=3, end_col_offset_idx=4, text='Niki Parmar*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=4, end_col_offset_idx=5, text='Jakob Uszkoreit*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=0, end_col_offset_idx=1, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=1, end_col_offset_idx=2, text='Google Brain', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=2, end_col_offset_idx=3, text='Google Brain', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=3, end_col_offset_idx=4, text='Google Research', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=4, end_col_offset_idx=5, text='Google Research', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=0, end_col_offset_idx=1, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=1, end_col_offset_idx=2, text='awvansing@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=2, end_col_offset_idx=3, text='tooming@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=3, end_col_offset_idx=4, text='nikip@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=4, end_col_offset_idx=5, text='nikip@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=0, end_col_offset_idx=1, text='3', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=1, end_col_offset_idx=2, text='Ilion Jones*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=2, end_col_offset_idx=3, text='Aidan N. Gomez*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=3, end_col_offset_idx=4, text='Lukasz Kaiser*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=4, end_col_offset_idx=5, text='Lukasz Kaiser*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=0, end_col_offset_idx=1, text='4', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=1, end_col_offset_idx=2, text='Google Research', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=2, end_col_offset_idx=3, text='University of Toronto', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=3, end_col_offset_idx=4, text='Google Brain', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=4, end_col_offset_idx=5, text='Google Brain', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=0, end_col_offset_idx=1, text='5', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=1, end_col_offset_idx=2, text='11ion@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=2, end_col_offset_idx=3, text='aidan@cs.toronto.edu', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=3, end_col_offset_idx=4, text='lukaszkaiser@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=4, end_col_offset_idx=5, text='lukaszkaiser@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=0, end_col_offset_idx=1, text='6', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=1, end_col_offset_idx=2, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=2, end_col_offset_idx=3, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=3, end_col_offset_idx=4, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=4, end_col_offset_idx=5, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=5, end_col_offset_idx=6, text='<', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=6, end_col_offset_idx=7, text='end_of_text', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=0, end_col_offset_idx=1, text='', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=1, end_col_offset_idx=2, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=2, end_col_offset_idx=3, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=3, end_col_offset_idx=4, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=4, end_col_offset_idx=5, text='3', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=0, end_col_offset_idx=1, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=1, end_col_offset_idx=2, text='Ashish Vaswani*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=2, end_col_offset_idx=3, text='Noam Shazeer*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=3, end_col_offset_idx=4, text='Niki Parmar*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=4, end_col_offset_idx=5, text='Jakob Uszkoreit*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=0, end_col_offset_idx=1, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=1, end_col_offset_idx=2, text='Google Brain', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=2, end_col_offset_idx=3, text='Google Brain', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=3, end_col_offset_idx=4, text='Google Research', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=4, end_col_offset_idx=5, text='Google Research', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=0, end_col_offset_idx=1, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=1, end_col_offset_idx=2, text='awvansing@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=2, end_col_offset_idx=3, text='tooming@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=3, end_col_offset_idx=4, text='nikip@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=4, end_col_offset_idx=5, text='nikip@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=0, end_col_offset_idx=1, text='3', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=1, end_col_offset_idx=2, text='Ilion Jones*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=2, end_col_offset_idx=3, text='Aidan N. Gomez*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=3, end_col_offset_idx=4, text='Lukasz Kaiser*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=4, end_col_offset_idx=5, text='Lukasz Kaiser*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=0, end_col_offset_idx=1, text='4', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=1, end_col_offset_idx=2, text='Google Research', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=2, end_col_offset_idx=3, text='University of Toronto', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=3, end_col_offset_idx=4, text='Google Brain', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=4, end_col_offset_idx=5, text='Google Brain', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=0, end_col_offset_idx=1, text='5', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=1, end_col_offset_idx=2, text='11ion@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=2, end_col_offset_idx=3, text='aidan@cs.toronto.edu', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=3, end_col_offset_idx=4, text='lukaszkaiser@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=4, end_col_offset_idx=5, text='lukaszkaiser@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=0, end_col_offset_idx=1, text='6', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=1, end_col_offset_idx=2, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=2, end_col_offset_idx=3, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=3, end_col_offset_idx=4, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=4, end_col_offset_idx=5, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=5, end_col_offset_idx=6, text='<', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=6, end_col_offset_idx=7, text='end_of_text', column_header=False, row_header=False, row_section=False)], num_rows=8, num_cols=5, grid=[[TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=0, end_col_offset_idx=1, text='', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=1, end_col_offset_idx=2, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=2, end_col_offset_idx=3, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=3, end_col_offset_idx=4, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=4, end_col_offset_idx=5, text='3', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=0, end_col_offset_idx=1, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=1, end_col_offset_idx=2, text='Ashish Vaswani*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=2, end_col_offset_idx=3, text='Noam Shazeer*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=3, end_col_offset_idx=4, text='Niki Parmar*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=4, end_col_offset_idx=5, text='Jakob Uszkoreit*', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=0, end_col_offset_idx=1, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=1, end_col_offset_idx=2, text='Google Brain', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=2, end_col_offset_idx=3, text='Google Brain', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=3, end_col_offset_idx=4, text='Google Research', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=4, end_col_offset_idx=5, text='Google Research', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=0, end_col_offset_idx=1, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=1, end_col_offset_idx=2, text='awvansing@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=2, end_col_offset_idx=3, text='tooming@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=3, end_col_offset_idx=4, text='nikip@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=4, end_col_offset_idx=5, text='nikip@google.com', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=0, end_col_offset_idx=1, text='3', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=1, end_col_offset_idx=2, text='Ilion Jones*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=2, end_col_offset_idx=3, text='Aidan N. Gomez*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=3, end_col_offset_idx=4, text='Lukasz Kaiser*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=4, end_col_offset_idx=5, text='Lukasz Kaiser*', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=0, end_col_offset_idx=1, text='4', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=1, end_col_offset_idx=2, text='Google Research', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=2, end_col_offset_idx=3, text='University of Toronto', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=3, end_col_offset_idx=4, text='Google Brain', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=4, end_col_offset_idx=5, text='Google Brain', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=0, end_col_offset_idx=1, text='5', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=1, end_col_offset_idx=2, text='11ion@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=2, end_col_offset_idx=3, text='aidan@cs.toronto.edu', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=3, end_col_offset_idx=4, text='lukaszkaiser@google.com', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=4, end_col_offset_idx=5, text='lukaszkaiser@google.com', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=0, end_col_offset_idx=1, text='6', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=1, end_col_offset_idx=2, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=2, end_col_offset_idx=3, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=3, end_col_offset_idx=4, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=4, end_col_offset_idx=5, text='Illia Polosukhin*', column_header=False, row_header=False, row_section=False)]])), TableItem(self_ref='#/tables/1', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TABLE: 'table'>, prov=[], captions=[], references=[], footnotes=[], image=None, data=TableData(table_cells=[TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=0, end_col_offset_idx=1, text='', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=1, end_col_offset_idx=2, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=0, end_col_offset_idx=1, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=1, end_col_offset_idx=2, text='1 Introduction', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=0, end_col_offset_idx=1, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=1, end_col_offset_idx=2, text='Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [25, 21, 5].', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=0, end_col_offset_idx=1, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=1, end_col_offset_idx=2, text='Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [3, 12, 24, 15].', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=0, end_col_offset_idx=1, text='3', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=1, end_col_offset_idx=2, text='Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t, as a function of the previous hidden state h t -1 and the input for position t.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=0, end_col_offset_idx=1, text='4', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=1, end_col_offset_idx=2, text='This inherently sequential nature precludes parallelization within training examples, which become critical as longer sequence lengths, as memory constraints limit batching across examples.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=0, end_col_offset_idx=1, text='5', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=1, end_col_offset_idx=2, text='Recent works have achieved significant improvements to computational efficiency through factorization tricks [22] and computational computation [31] while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=0, end_col_offset_idx=1, text='6', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=1, end_col_offset_idx=2, text='Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [21, 19]. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=8, end_row_offset_idx=9, start_col_offset_idx=0, end_col_offset_idx=1, text='7', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=8, end_row_offset_idx=9, start_col_offset_idx=1, end_col_offset_idx=2, text='In this work we propose the Transformer, a model architecture eschewing recurrent and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=8, end_row_offset_idx=9, start_col_offset_idx=2, end_col_offset_idx=3, text='<', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=8, end_row_offset_idx=9, start_col_offset_idx=3, end_col_offset_idx=4, text='end_of_text', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=0, end_col_offset_idx=1, text='', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=1, end_col_offset_idx=2, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=0, end_col_offset_idx=1, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=1, end_col_offset_idx=2, text='1 Introduction', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=0, end_col_offset_idx=1, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=1, end_col_offset_idx=2, text='Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [25, 21, 5].', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=0, end_col_offset_idx=1, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=1, end_col_offset_idx=2, text='Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [3, 12, 24, 15].', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=0, end_col_offset_idx=1, text='3', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=1, end_col_offset_idx=2, text='Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t, as a function of the previous hidden state h t -1 and the input for position t.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=0, end_col_offset_idx=1, text='4', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=1, end_col_offset_idx=2, text='This inherently sequential nature precludes parallelization within training examples, which become critical as longer sequence lengths, as memory constraints limit batching across examples.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=0, end_col_offset_idx=1, text='5', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=1, end_col_offset_idx=2, text='Recent works have achieved significant improvements to computational efficiency through factorization tricks [22] and computational computation [31] while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=0, end_col_offset_idx=1, text='6', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=1, end_col_offset_idx=2, text='Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [21, 19]. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=8, end_row_offset_idx=9, start_col_offset_idx=0, end_col_offset_idx=1, text='7', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=8, end_row_offset_idx=9, start_col_offset_idx=1, end_col_offset_idx=2, text='In this work we propose the Transformer, a model architecture eschewing recurrent and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=8, end_row_offset_idx=9, start_col_offset_idx=2, end_col_offset_idx=3, text='<', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=8, end_row_offset_idx=9, start_col_offset_idx=3, end_col_offset_idx=4, text='end_of_text', column_header=False, row_header=False, row_section=False)], num_rows=9, num_cols=2, grid=[[TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=0, end_col_offset_idx=1, text='', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=1, end_col_offset_idx=2, text='0', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=0, end_col_offset_idx=1, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=1, end_col_offset_idx=2, text='1 Introduction', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=0, end_col_offset_idx=1, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=1, end_col_offset_idx=2, text='Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [25, 21, 5].', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=0, end_col_offset_idx=1, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=1, end_col_offset_idx=2, text='Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [3, 12, 24, 15].', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=0, end_col_offset_idx=1, text='3', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=4, end_row_offset_idx=5, start_col_offset_idx=1, end_col_offset_idx=2, text='Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t, as a function of the previous hidden state h t -1 and the input for position t.', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=0, end_col_offset_idx=1, text='4', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=5, end_row_offset_idx=6, start_col_offset_idx=1, end_col_offset_idx=2, text='This inherently sequential nature precludes parallelization within training examples, which become critical as longer sequence lengths, as memory constraints limit batching across examples.', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=0, end_col_offset_idx=1, text='5', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=6, end_row_offset_idx=7, start_col_offset_idx=1, end_col_offset_idx=2, text='Recent works have achieved significant improvements to computational efficiency through factorization tricks [22] and computational computation [31] while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=0, end_col_offset_idx=1, text='6', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=7, end_row_offset_idx=8, start_col_offset_idx=1, end_col_offset_idx=2, text='Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [21, 19]. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=8, end_row_offset_idx=9, start_col_offset_idx=0, end_col_offset_idx=1, text='7', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=8, end_row_offset_idx=9, start_col_offset_idx=1, end_col_offset_idx=2, text='In this work we propose the Transformer, a model architecture eschewing recurrent and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.', column_header=False, row_header=False, row_section=False)]])), TableItem(self_ref='#/tables/2', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TABLE: 'table'>, prov=[], captions=[], references=[], footnotes=[], image=None, data=TableData(table_cells=[TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=0, end_col_offset_idx=1, text='', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=1, end_col_offset_idx=2, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=0, end_col_offset_idx=1, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=1, end_col_offset_idx=2, text='Encoder: The encoder is composed of a stack of N = 6 identical layers.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=0, end_col_offset_idx=1, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=1, end_col_offset_idx=2, text='Each layer has two sub-layers. The first is a multi-head self-attention', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=0, end_col_offset_idx=1, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=1, end_col_offset_idx=2, text='mechanism, and the second is a simple, position-wise fully connected network.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=2, end_col_offset_idx=3, text='<', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=3, end_col_offset_idx=4, text='end_of_text', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=0, end_col_offset_idx=1, text='', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=1, end_col_offset_idx=2, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=0, end_col_offset_idx=1, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=1, end_col_offset_idx=2, text='Encoder: The encoder is composed of a stack of N = 6 identical layers.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=0, end_col_offset_idx=1, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=1, end_col_offset_idx=2, text='Each layer has two sub-layers. The first is a multi-head self-attention', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=0, end_col_offset_idx=1, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=1, end_col_offset_idx=2, text='mechanism, and the second is a simple, position-wise fully connected network.', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=2, end_col_offset_idx=3, text='<', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=3, end_col_offset_idx=4, text='end_of_text', column_header=False, row_header=False, row_section=False)], num_rows=4, num_cols=2, grid=[[TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=0, end_col_offset_idx=1, text='', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=0, end_row_offset_idx=1, start_col_offset_idx=1, end_col_offset_idx=2, text='0', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=0, end_col_offset_idx=1, text='0', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=1, end_row_offset_idx=2, start_col_offset_idx=1, end_col_offset_idx=2, text='Encoder: The encoder is composed of a stack of N = 6 identical layers.', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=0, end_col_offset_idx=1, text='1', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=2, end_row_offset_idx=3, start_col_offset_idx=1, end_col_offset_idx=2, text='Each layer has two sub-layers. The first is a multi-head self-attention', column_header=False, row_header=False, row_section=False)], [TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=0, end_col_offset_idx=1, text='2', column_header=False, row_header=False, row_section=False), TableCell(bbox=None, row_span=1, col_span=1, start_row_offset_idx=3, end_row_offset_idx=4, start_col_offset_idx=1, end_col_offset_idx=2, text='mechanism, and the second is a simple, position-wise fully connected network.', column_header=False, row_header=False, row_section=False)]]))], key_value_items=[], form_items=[], pages={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cref='#/tables/0'\n",
      "cref='#/tables/1'\n",
      "cref='#/tables/2'\n"
     ]
    }
   ],
   "source": [
    "for item in res.document.body.children:\n",
    "    print(item)\n",
    "    # print(type(item))\n",
    "    \n",
    "# item = res.document.pictures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = item.get_image(res.document)\n",
    "# print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling_core.types.doc import DocItemLabel, ImageRefMode\n",
    "from docling_core.types.doc.document import DEFAULT_EXPORT_LABELS\n",
    "\n",
    "res.document.save_as_html(\n",
    "    filename=Path(\"./3_docling_vlm_test_result.html\"),\n",
    "    image_mode=ImageRefMode.REFERENCED,\n",
    "    labels=[*DEFAULT_EXPORT_LABELS, DocItemLabel.FOOTNOTE],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<DocItemLabel.CHECKBOX_SELECTED: 'checkbox_selected'>,\n",
       " <DocItemLabel.CHECKBOX_UNSELECTED: 'checkbox_unselected'>,\n",
       " <DocItemLabel.CODE: 'code'>,\n",
       " <DocItemLabel.DOCUMENT_INDEX: 'document_index'>,\n",
       " <DocItemLabel.FORMULA: 'formula'>,\n",
       " <DocItemLabel.LIST_ITEM: 'list_item'>,\n",
       " <DocItemLabel.PAGE_FOOTER: 'page_footer'>,\n",
       " <DocItemLabel.PAGE_HEADER: 'page_header'>,\n",
       " <DocItemLabel.PARAGRAPH: 'paragraph'>,\n",
       " <DocItemLabel.PICTURE: 'picture'>,\n",
       " <DocItemLabel.REFERENCE: 'reference'>,\n",
       " <DocItemLabel.SECTION_HEADER: 'section_header'>,\n",
       " <DocItemLabel.TABLE: 'table'>,\n",
       " <DocItemLabel.TEXT: 'text'>,\n",
       " <DocItemLabel.TITLE: 'title'>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_EXPORT_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling",
   "language": "python",
   "name": "docling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
