{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm/lib/python3.10/site-packages/pydantic/_internal/_fields.py:152: UserWarning: Field \"model_weight_dir\" in Settings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "from typing import ClassVar, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BatchFeature\n",
    "from transformers.models.qwen2_vl import Qwen2VLProcessor\n",
    "from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize\n",
    "\n",
    "from colpali_engine.utils.processing_utils import BaseVisualRetrieverProcessor\n",
    "from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor\n",
    "\n",
    "from config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultimodalColQwen_2_5_Processor(ColQwen2_5_Processor):\n",
    "#     def process_images(self, images: List[Image.Image], context_prompts: Optional[List[str]] = None) -> BatchFeature:\n",
    "#         \"\"\"\n",
    "#         Process images for ColQwen2.5.\n",
    "#         \"\"\"\n",
    "\n",
    "#         if context_prompts:\n",
    "#             texts_doc = context_prompts\n",
    "#         else:\n",
    "#             texts_doc = [self.visual_prompt_prefix] * len(images)\n",
    "#         images = [image.convert(\"RGB\") for image in images]\n",
    "\n",
    "#         batch_doc = self(\n",
    "#             text=texts_doc,\n",
    "#             images=images,\n",
    "#             padding=\"longest\",\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "\n",
    "#         # NOTE: The following adjustment ensures correct behavior with DDP on multiple GPUs.\n",
    "#         offsets = batch_doc[\"image_grid_thw\"][:, 1] * batch_doc[\"image_grid_thw\"][:, 2]  # (batch_size,)\n",
    "\n",
    "#         # Split the pixel_values tensor into a list of tensors, one per image\n",
    "#         pixel_values = list(\n",
    "#             torch.split(batch_doc[\"pixel_values\"], offsets.tolist())\n",
    "#         )  # [(num_patches_image_0, pixel_values), ..., (num_patches_image_n, pixel_values)]\n",
    "\n",
    "#         # Pad the list of pixel_value tensors to the same length along the sequence dimension\n",
    "#         batch_doc[\"pixel_values\"] = torch.nn.utils.rnn.pad_sequence(\n",
    "#             pixel_values, batch_first=True\n",
    "#         )  # (batch_size, max_num_patches, pixel_values)\n",
    "\n",
    "#         return batch_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c575f8756b4e92919540e13af38e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ColQwen2_5 were not initialized from the model checkpoint at /Users/id4thomas/models/Qwen2.5-VL-3B-Instruct and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_dir = os.path.join(\n",
    "    settings.model_weight_dir, \"Qwen2.5-VL-3B-Instruct\"\n",
    ")\n",
    "adapter_dir = os.path.join(\n",
    "    settings.model_weight_dir, \"embedding/colnomic-embed-multimodal-3b\"\n",
    ")\n",
    "\n",
    "# https://github.com/QwenLM/Qwen2.5-VL/issues/760#issuecomment-2657856186\n",
    "model = ColQwen2_5.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"mps\",\n",
    "    attn_implementation=\"eager\",\n",
    ").eval()\n",
    "\n",
    "model.load_adapter(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NomicColQwen_2_5_Processor(ColQwen2_5_Processor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        max_num_visual_tokens: int = 768,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "        self.max_num_visual_tokens = max_num_visual_tokens\n",
    "        self.factor = 28\n",
    "        self.min_pixels = 4 * 28 * 28\n",
    "        self.max_pixels = self.max_num_visual_tokens * 28 * 28\n",
    "\n",
    "        self.image_processor.min_pixels = self.min_pixels\n",
    "        self.image_processor.max_pixels = self.max_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: max_num_visual_tokens. \n"
     ]
    }
   ],
   "source": [
    "processor = NomicColQwen_2_5_Processor.from_pretrained(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image. The following is text related to the image: 랜덤한 이미지 1<|im_end|><|endoftext|>',\n",
       " '<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image. The following is text related to the image: 랜덤한 이미지 2<|im_end|><|endoftext|>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context_template = \"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image.<|im_end|><|endoftext|>\"\n",
    "context_template = '''<|im_start|>user\n",
    "<|vision_start|><|image_pad|><|vision_end|>Describe the image. The following is text related to the image: {}<|im_end|><|endoftext|>'''\n",
    "\n",
    "images = [\n",
    "    Image.new(\"RGB\", (128, 128), color=\"white\"),\n",
    "    Image.new(\"RGB\", (64, 32), color=\"black\"),\n",
    "]\n",
    "context_contents = [\n",
    "    \"랜덤한 이미지 1\",\n",
    "    \"랜덤한 이미지 2\",\n",
    "]\n",
    "context_prompts = [\n",
    "    context_template.format(x) for x in context_contents\n",
    "]\n",
    "context_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = processor.process_images(images=images, context_prompts=context_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,    872,    198, 151652, 151655, 151655, 151655, 151655, 151655,\n",
       "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "         151655, 151655, 151653,  74785,    279,   2168,     13,    576,   2701,\n",
       "            374,   1467,   5435,    311,    279,   2168,     25,   5140,    252,\n",
       "            250, 144452,  23573,  90667,  21329,    220,     16, 151645, 151643],\n",
       "        [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151644,    872,    198, 151652, 151655, 151655, 151655, 151655,\n",
       "         151655, 151655, 151653,  74785,    279,   2168,     13,    576,   2701,\n",
       "            374,   1467,   5435,    311,    279,   2168,     25,   5140,    252,\n",
       "            250, 144452,  23573,  90667,  21329,    220,     17, 151645, 151643]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "         [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "         [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "         ...,\n",
       "         [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "         [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "         [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459]],\n",
       "\n",
       "        [[-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
       "         [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
       "         [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), 'image_grid_thw': tensor([[ 1, 10, 10],\n",
       "        [ 1,  4,  6]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_embeddings = model(**batch.to(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 54, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
