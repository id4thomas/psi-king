{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm/lib/python3.10/site-packages/pydantic/_internal/_fields.py:152: UserWarning: Field \"model_weight_dir\" in Settings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "from typing import ClassVar, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BatchFeature\n",
    "from transformers.models.qwen2_vl import Qwen2VLProcessor\n",
    "from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize\n",
    "\n",
    "from colpali_engine.utils.processing_utils import BaseVisualRetrieverProcessor\n",
    "from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor\n",
    "\n",
    "from config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276d10d6d55245b3967189d9ea7b0c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ColQwen2_5 were not initialized from the model checkpoint at /Users/id4thomas/models/Qwen2.5-VL-3B-Instruct and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_dir = os.path.join(\n",
    "    settings.model_weight_dir, \"Qwen2.5-VL-3B-Instruct\"\n",
    ")\n",
    "adapter_dir = os.path.join(\n",
    "    settings.model_weight_dir, \"embedding/colnomic-embed-multimodal-3b\"\n",
    ")\n",
    "\n",
    "# https://github.com/QwenLM/Qwen2.5-VL/issues/760#issuecomment-2657856186\n",
    "model = ColQwen2_5.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"mps\",\n",
    "    attn_implementation=\"eager\",\n",
    "    # local_files_only=True,\n",
    ").eval()\n",
    "\n",
    "model.load_adapter(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NomicColQwen_2_5_Processor(ColQwen2_5_Processor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        max_num_visual_tokens: int = 768,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "        self.max_num_visual_tokens = max_num_visual_tokens\n",
    "        self.factor = 28\n",
    "        self.min_pixels = 4 * 28 * 28\n",
    "        self.max_pixels = self.max_num_visual_tokens * 28 * 28\n",
    "\n",
    "        self.image_processor.min_pixels = self.min_pixels\n",
    "        self.image_processor.max_pixels = self.max_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: max_num_visual_tokens. \n"
     ]
    }
   ],
   "source": [
    "processor = NomicColQwen_2_5_Processor.from_pretrained(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image. The following is text related to the image: 랜덤한 이미지 1<|im_end|><|endoftext|>',\n",
       " '<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image. The following is text related to the image: 랜덤한 이미지 2<|im_end|><|endoftext|>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context_template = \"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image.<|im_end|><|endoftext|>\"\n",
    "context_template = '''<|im_start|>user\n",
    "<|vision_start|><|image_pad|><|vision_end|>Describe the image. The following is text related to the image: {}<|im_end|><|endoftext|>'''\n",
    "\n",
    "images = [\n",
    "    Image.new(\"RGB\", (128, 128), color=\"white\"),\n",
    "    Image.new(\"RGB\", (64, 32), color=\"black\"),\n",
    "]\n",
    "context_contents = [\n",
    "    \"랜덤한 이미지 1\",\n",
    "    \"랜덤한 이미지 2\",\n",
    "]\n",
    "context_prompts = [\n",
    "    context_template.format(x) for x in context_contents\n",
    "]\n",
    "context_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = processor.process_images(images=images, context_prompts=context_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,    872,    198, 151652, 151655, 151655, 151655, 151655, 151655,\n",
       "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "         151655, 151655, 151653,  74785,    279,   2168,     13,    576,   2701,\n",
       "            374,   1467,   5435,    311,    279,   2168,     25,   5140,    252,\n",
       "            250, 144452,  23573,  90667,  21329,    220,     16, 151645, 151643],\n",
       "        [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151644,    872,    198, 151652, 151655, 151655, 151655, 151655,\n",
       "         151655, 151655, 151653,  74785,    279,   2168,     13,    576,   2701,\n",
       "            374,   1467,   5435,    311,    279,   2168,     25,   5140,    252,\n",
       "            250, 144452,  23573,  90667,  21329,    220,     17, 151645, 151643]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "         [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "         [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "         ...,\n",
       "         [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "         [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "         [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459]],\n",
       "\n",
       "        [[-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
       "         [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
       "         [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), 'image_grid_thw': tensor([[ 1, 10, 10],\n",
       "        [ 1,  4,  6]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_embeddings = model(**batch.to(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 54, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using bentoml served model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "from PIL import Image\n",
    "\n",
    "from model_serving.nomic_colnomic.interfaces import ImagePayload\n",
    "from model_serving.nomic_colnomic.utils import convert_pil_to_b64_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image. The following is text related to the image: 랜덤한 이미지 1<|im_end|><|endoftext|>',\n",
       " '<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image. The following is text related to the image: 랜덤한 이미지 2<|im_end|><|endoftext|>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_template = '''<|im_start|>user\n",
    "<|vision_start|><|image_pad|><|vision_end|>Describe the image. The following is text related to the image: {}<|im_end|><|endoftext|>'''\n",
    "\n",
    "images = [\n",
    "    Image.new(\"RGB\", (128, 128), color=\"white\"),\n",
    "    Image.new(\"RGB\", (64, 32), color=\"black\"),\n",
    "]\n",
    "context_contents = [\n",
    "    \"랜덤한 이미지 1\",\n",
    "    \"랜덤한 이미지 2\",\n",
    "]\n",
    "context_prompts = [\n",
    "    context_template.format(x) for x in context_contents\n",
    "]\n",
    "context_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = [\n",
    "    ImagePayload(\n",
    "        url=convert_pil_to_b64_image(image),\n",
    "        text=text\n",
    "    )\n",
    "    for image, text in zip(images, context_prompts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38.26011658 37.82732391]\n",
      " [37.84283447 37.98073578]]\n"
     ]
    }
   ],
   "source": [
    "with bentoml.SyncHTTPClient(\"http://localhost:3000\") as client:\n",
    "    embeddings = client.embed_images(items=payloads)\n",
    "    query_embeddings = client.embed_queries(items=context_prompts)\n",
    "\n",
    "    scores = client.score_embeddings(\n",
    "        image_embeddings=embeddings,\n",
    "        query_embeddings=query_embeddings,\n",
    "    )\n",
    "\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
