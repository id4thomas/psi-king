{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "core_src_dir = os.path.join(parent_dir, \"src/psiking\")\n",
    "sys.path.append(core_src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Core Schemas\n",
    "from core.base.schema import Document, TextNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read data\n",
    "* read BEIR data & convert to Document\n",
    "    * https://huggingface.co/BeIR\n",
    "* test with `scifact` dataset\n",
    "    * 5K passages\n",
    "    * https://huggingface.co/datasets/BeIR/scifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement Custom Readers\n",
    "from core.reader.base import BaseReader\n",
    "\n",
    "class QuoraDataReader(BaseReader):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def read(\n",
    "        self,\n",
    "        data: dict,\n",
    "        extra_info: Optional[dict] = None,\n",
    "    ) -> Optional[Document]:\n",
    "        \"\"\"Data format\n",
    "        ['_id', 'title', 'text', 'metadata']\n",
    "        \"\"\"\n",
    "        metadata = extra_info or {}\n",
    "        \n",
    "        text = data.get('text', '')\n",
    "        if not text:\n",
    "            return None\n",
    "        node = TextNode(\n",
    "            text=text,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        return Document(\n",
    "            nodes=[node],\n",
    "            metadata={\n",
    "                \"source_id\": data['_id'],\n",
    "                \"title\": data['title'],\n",
    "                **metadata\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        file_path: str | Path,\n",
    "        extra_info: Optional[dict] = None\n",
    "    ) -> List[Document]:\n",
    "        metadata = extra_info or {}\n",
    "        \n",
    "        documents = []\n",
    "        with jsonlines.open(file_path) as reader:\n",
    "            for data in reader:\n",
    "                document = self.read(\n",
    "                    data,\n",
    "                    extra_info={\n",
    "                        **metadata\n",
    "                    }\n",
    "                )\n",
    "                if document:\n",
    "                    documents.append(document)\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5183\n"
     ]
    }
   ],
   "source": [
    "document_path = os.path.join(settings.data_dir, \"beir/scifact/corpus.jsonl\")\n",
    "\n",
    "reader = QuoraDataReader()\n",
    "documents = reader.run(document_path, extra_info={\"source_file\": \"beir-scifact-corpus\"})\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='d1c7ffe5-7176-4268-ae65-59d0574bbb26', metadata={'source_id': '4983', 'title': 'Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.', 'source_file': 'beir-scifact-corpus'}, nodes=[TextNode(id_='ad4fd803-965e-404a-a8c2-a9076bd5189f', metadata={'source_file': 'beir-scifact-corpus'}, text_type=<TextType.PLAIN: 'plain'>, label=<TextLabel.PLAIN: 'plain'>, resource=MediaResource(data=None, text='Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient, to calculate relative anisotropy, and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development, early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coefficient at 28 wk was high, 1.8 microm2/ms, and decreased toward term to 1.2 microm2/ms. In the posterior limb of the internal capsule, the mean apparent diffusion coefficients at both times were similar (1.2 versus 1.1 microm2/ms). Relative anisotropy was higher the closer birth was to term with greater absolute values in the internal capsule than in the central white matter. Preterm infants at term showed higher mean diffusion coefficients in the central white matter (1.4 +/- 0.24 versus 1.15 +/- 0.09 microm2/ms, p = 0.016) and lower relative anisotropy in both areas compared with full-term infants (white matter, 10.9 +/- 0.6 versus 22.9 +/- 3.0%, p = 0.001; internal capsule, 24.0 +/- 4.44 versus 33.1 +/- 0.6% p = 0.006). Nonmyelinated fibers in the corpus callosum were visible by diffusion tensor MRI as early as 28 wk; full-term and preterm infants at term showed marked differences in white matter fiber organization. The data indicate that quantitative assessment of water diffusion by diffusion tensor MRI provides insight into microstructural development in cerebral white matter in living infants.', path=None, url=None, mimetype=None))]),\n",
       " Document(id_='b81d1e87-830e-4d35-85c5-53161402f025', metadata={'source_id': '5836', 'title': 'Induction of myelodysplasia by myeloid-derived suppressor cells.', 'source_file': 'beir-scifact-corpus'}, nodes=[TextNode(id_='d95bf024-e84d-4294-9d08-2add1a350a24', metadata={'source_file': 'beir-scifact-corpus'}, text_type=<TextType.PLAIN: 'plain'>, label=<TextLabel.PLAIN: 'plain'>, resource=MediaResource(data=None, text='Myelodysplastic syndromes (MDS) are age-dependent stem cell malignancies that share biological features of activated adaptive immune response and ineffective hematopoiesis. Here we report that myeloid-derived suppressor cells (MDSC), which are classically linked to immunosuppression, inflammation, and cancer, were markedly expanded in the bone marrow of MDS patients and played a pathogenetic role in the development of ineffective hematopoiesis. These clonally distinct MDSC overproduce hematopoietic suppressive cytokines and function as potent apoptotic effectors targeting autologous hematopoietic progenitors. Using multiple transfected cell models, we found that MDSC expansion is driven by the interaction of the proinflammatory molecule S100A9 with CD33. These 2 proteins formed a functional ligand/receptor pair that recruited components to CD33’s immunoreceptor tyrosine-based inhibition motif (ITIM), inducing secretion of the suppressive cytokines IL-10 and TGF-β by immature myeloid cells. S100A9 transgenic mice displayed bone marrow accumulation of MDSC accompanied by development of progressive multilineage cytopenias and cytological dysplasia. Importantly, early forced maturation of MDSC by either all-trans-retinoic acid treatment or active immunoreceptor tyrosine-based activation motif–bearing (ITAM-bearing) adapter protein (DAP12) interruption of CD33 signaling rescued the hematologic phenotype. These findings indicate that primary bone marrow expansion of MDSC driven by the S100A9/CD33 pathway perturbs hematopoiesis and contributes to the development of MDS.', path=None, url=None, mimetype=None))])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Run Splitter\n",
    "* simple inefficient splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5183\n"
     ]
    }
   ],
   "source": [
    "# 3. Run Splitter\n",
    "from core.splitter.text.langchain_text_splitters import LangchainRecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = LangchainRecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 128\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for document in documents:\n",
    "    document_chunks = []\n",
    "    source_id = document.id_\n",
    "    for i, node in enumerate(document.nodes):\n",
    "        if isinstance(node, TextNode):\n",
    "            split_nodes = splitter.run(node)\n",
    "        else:\n",
    "            split_nodes = [node]\n",
    "        chunk = Document(\n",
    "            nodes=split_nodes,\n",
    "            metadata={\"source_id\": source_id}\n",
    "        )\n",
    "        document_chunks.append(chunk)\n",
    "    chunks.extend(document_chunks)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Format (Prepare Embedding Input)\n",
    "from core.formatter.document.simple import SimpleTextOnlyFormatter\n",
    "\n",
    "# use default templates\n",
    "formatter = SimpleTextOnlyFormatter()\n",
    "formatted_texts = formatter.run(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5183\n"
     ]
    }
   ],
   "source": [
    "def select_embedding_input_idxs(texts: str, min_length: int = 20):\n",
    "    return [i for i, x in enumerate(texts) if len(x.strip())>min_length]\n",
    "\n",
    "embedding_input_idxs = select_embedding_input_idxs(\n",
    "    texts=formatted_texts,\n",
    "    min_length=20\n",
    ")\n",
    "print(len(embedding_input_idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baai/bge-m3\n"
     ]
    }
   ],
   "source": [
    "# 5. Embed\n",
    "from openai import OpenAI\n",
    "from core.embedder.openai.text_embedder import OpenAITextEmbedder\n",
    "\n",
    "print(settings.openai_embedding_model)\n",
    "client = OpenAI(\n",
    "    base_url=settings.openai_embedding_base_url,\n",
    "    api_key=settings.openai_embedding_api_key\n",
    ")\n",
    "\n",
    "embedder = OpenAITextEmbedder(\n",
    "    client = client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432/432 [01:29<00:00,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_inputs = [formatted_texts[x] for x in embedding_input_idxs]\n",
    "embeddings = embedder.run(\n",
    "    texts=embedding_inputs,\n",
    "    model=settings.openai_embedding_model,\n",
    "    batch_size = 12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5183 1024\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings), len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Add to VectorStore\n",
    "* qdrant single vector vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Add to VectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from core.storage.vectorstore.qdrant import QdrantSingleVectorStore\n",
    "\n",
    "# initialize client\n",
    "client = QdrantClient(\":memory:\")\n",
    "collection_name = \"beir-scifact\"\n",
    "\n",
    "vector_store = QdrantSingleVectorStore(\n",
    "    collection_name=collection_name,\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models\n",
    "\n",
    "embedding_dim = len(embeddings[0])\n",
    "\n",
    "vector_store.create_collection(\n",
    "    on_disk_payload=True,  # store the payload on disk\n",
    "    vectors_config = models.VectorParams(\n",
    "        size=embedding_dim,\n",
    "        distance=models.Distance.COSINE,\n",
    "        on_disk=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add(\n",
    "    documents=[chunks[x] for x in embedding_input_idxs],\n",
    "    embeddings=embeddings,\n",
    "    metadata_keys=[\"source_file\", \"source_id\", \"title\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"status\": \"green\",\n",
      "    \"optimizer_status\": \"ok\",\n",
      "    \"vectors_count\": null,\n",
      "    \"indexed_vectors_count\": 0,\n",
      "    \"points_count\": 5183,\n",
      "    \"segments_count\": 1,\n",
      "    \"config\": {\n",
      "        \"params\": {\n",
      "            \"vectors\": {\n",
      "                \"size\": 1024,\n",
      "                \"distance\": \"Cosine\",\n",
      "                \"hnsw_config\": null,\n",
      "                \"quantization_config\": null,\n",
      "                \"on_disk\": true,\n",
      "                \"datatype\": null,\n",
      "                \"multivector_config\": null\n",
      "            },\n",
      "            \"shard_number\": null,\n",
      "            \"sharding_method\": null,\n",
      "            \"replication_factor\": null,\n",
      "            \"write_consistency_factor\": null,\n",
      "            \"read_fan_out_factor\": null,\n",
      "            \"on_disk_payload\": null,\n",
      "            \"sparse_vectors\": null\n",
      "        },\n",
      "        \"hnsw_config\": {\n",
      "            \"m\": 16,\n",
      "            \"ef_construct\": 100,\n",
      "            \"full_scan_threshold\": 10000,\n",
      "            \"max_indexing_threads\": 0,\n",
      "            \"on_disk\": null,\n",
      "            \"payload_m\": null\n",
      "        },\n",
      "        \"optimizer_config\": {\n",
      "            \"deleted_threshold\": 0.2,\n",
      "            \"vacuum_min_vector_number\": 1000,\n",
      "            \"default_segment_number\": 0,\n",
      "            \"max_segment_size\": null,\n",
      "            \"memmap_threshold\": null,\n",
      "            \"indexing_threshold\": 20000,\n",
      "            \"flush_interval_sec\": 5,\n",
      "            \"max_optimization_threads\": 1\n",
      "        },\n",
      "        \"wal_config\": {\n",
      "            \"wal_capacity_mb\": 32,\n",
      "            \"wal_segments_ahead\": 0\n",
      "        },\n",
      "        \"quantization_config\": null,\n",
      "        \"strict_mode_config\": null\n",
      "    },\n",
      "    \"payload_schema\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# check collection\n",
    "collection_info = vector_store._client.get_collection(\n",
    "    collection_name=vector_store.collection_name\n",
    ")\n",
    "print(collection_info.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check point\n",
    "points = vector_store._client.retrieve(\n",
    "    collection_name=vector_store.collection_name,\n",
    "    ids=[chunks[0].id_],\n",
    "    with_vectors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53c5cc53-0782-4beb-ae2e-07661f211716\n",
      "{'source_id': 'd1c7ffe5-7176-4268-ae65-59d0574bbb26'}\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(points[0].id)\n",
    "print(points[0].payload)\n",
    "print(len(points[0].vector))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling",
   "language": "python",
   "name": "docling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
