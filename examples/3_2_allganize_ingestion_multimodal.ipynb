{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# allganize-RAG-Evaluation data + multimodal search\n",
    "## Methodology\n",
    "1. \n",
    "\n",
    "## Env\n",
    "* model: `baai/bge-visualized` (bge-m3 weight)\n",
    "* data: real-life pdf files from `allganize-RAG-Evaluation-Dataset-KO`\n",
    "    * https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO\n",
    "    * use 10 'finance' domain files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "core_src_dir = os.path.join(parent_dir, \"src/psiking\")\n",
    "sys.path.append(core_src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Core Schemas\n",
    "from core.base.schema import Document, TextNode, ImageNode, TableNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read Data\n",
    "* 10 pdf files, convert to image with pdf2image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Load DoclingPDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLM MODEL: Qwen2.5-VL-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from docling.backend.docling_parse_v2_backend import DoclingParseV2DocumentBackend\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    AcceleratorOptions,\n",
    "    PdfPipelineOptions,\n",
    "    PictureDescriptionApiOptions,\n",
    "    TableStructureOptions,\n",
    "    TableFormerMode\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "from core.reader.pdf.docling_reader import DoclingPDFReader\n",
    "\n",
    "from src.docling_vllm_picture_description_pipeline import (\n",
    "    VLLMPictureDescriptionApiOptions,\n",
    "    VLLMPictureDescriptionPdfPipeline\n",
    ")\n",
    "\n",
    "# Initialize format options\n",
    "format_options = PdfPipelineOptions()\n",
    "\n",
    "format_options.accelerator_options = AcceleratorOptions(device=\"mps\")\n",
    "\n",
    "format_options.images_scale = 1.5\n",
    "format_options.generate_page_images = True\n",
    "format_options.generate_picture_images = True\n",
    "format_options.do_ocr = False\n",
    "\n",
    "# Image description\n",
    "print(\"VLM MODEL:\", settings.vlm_model)\n",
    "\n",
    "DESCRIPTION_INSTRUCTION = '''주어진 이미지에대해 2가지 정보를 반환합니다.\n",
    "* description: 최대 2문장 정도로 이미지에 대한 간결한 설명\n",
    "* text: 이미지 내에서 인식된 모든 텍스트\n",
    "다음 JSON 형식으로 반환하세요 {\"description\": str, \"text\": str}'''\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "    description: str\n",
    "    text: str\n",
    "\n",
    "image_description_options = VLLMPictureDescriptionApiOptions(\n",
    "    url=f\"{settings.vlm_base_url}/v1/chat/completions\",\n",
    "    params=dict(\n",
    "        model=settings.vlm_model,\n",
    "        seed=42,\n",
    "        max_completion_tokens=512,\n",
    "        temperature=0.9,\n",
    "        extra_body={\"guided_json\": ImageDescription.model_json_schema()}\n",
    "    ),\n",
    "    # prompt=\"이미지에 대해 최대 2문장 정도로 설명하고 있는 텍스트를 모두 추출하세요. 이미지에 정보가 없다면 설명 텍스트를 작성하지 않습니다. 인식 텍스트와 설명만 반환하세요.\",\n",
    "    prompt=DESCRIPTION_INSTRUCTION,\n",
    "    batch_size=6, # Not implemented inside\n",
    "    scale=0.9,\n",
    "    timeout=90,\n",
    "    min_coverage_area_threshold=0.01,\n",
    "    bitmap_area_threshold=0.1 # 10% of page area\n",
    ")\n",
    "\n",
    "format_options.do_picture_description = True\n",
    "# format_options.do_picture_description = False\n",
    "format_options.enable_remote_services = True\n",
    "format_options.picture_description_options = image_description_options\n",
    "\n",
    "# TableStructure\n",
    "format_options.do_table_structure = True\n",
    "format_options.table_structure_options = TableStructureOptions(mode=TableFormerMode.ACCURATE)\n",
    "\n",
    "# Initialize Converter\n",
    "converter = DocumentConverter(\n",
    "    allowed_formats = [\n",
    "        InputFormat.PDF,\n",
    "    ],\n",
    "    format_options = {\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_cls=VLLMPictureDescriptionPdfPipeline,\n",
    "            pipeline_options = format_options,\n",
    "            backend = DoclingParseV2DocumentBackend\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize reader\n",
    "reader = DoclingPDFReader(converter=converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Load PDF fnames, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num files: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['★2019 제1회 증시콘서트 자료집_최종★.pdf',\n",
       " '240409(보도자료) 금융위 핀테크 투자 생태계 활성화 나선다.pdf',\n",
       " '2024년 3월_3. 향후 통화신용정책 방향.pdf',\n",
       " '133178946057443204_WP22-05.pdf',\n",
       " '240130(보도자료) 지방은행의 시중은행 전환시 인가방식 및 절차.pdf',\n",
       " '130292099630937500_KIFVIP2013-10.pdf',\n",
       " '2024년 3월_2. 통화신용정책 운영.pdf',\n",
       " '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf',\n",
       " '240320(보도자료) 금융권의 상생금융 추진현황.pdf',\n",
       " '한-호주 퇴직연금 포럼_책자(최종).pdf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF File directory\n",
    "pdf_dir = os.path.join(settings.data_dir, \"allganize-RAG-Evaluation-Dataset-KO/finance\")\n",
    "pdf_fnames =[x for x in os.listdir(pdf_dir) if x.endswith(\".pdf\")]\n",
    "print(\"num files:\", len(pdf_fnames))\n",
    "pdf_fnames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM IMAGES TO ANNOTATE 4\n",
      "NUM IMAGES TO ANNOTATE 11\n",
      "NUM IMAGES TO ANNOTATE 10\n",
      "NUM IMAGES TO ANNOTATE 13\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:32, 92.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM IMAGES TO ANNOTATE 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:40, 42.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM IMAGES TO ANNOTATE 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [02:14, 44.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'core.base.schema.TextNode'>\n",
      "<class 'core.base.schema.TextNode'>\n",
      "<class 'core.base.schema.TableNode'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert pages to image\n",
    "documents = []\n",
    "failed_fnames = []\n",
    "for doc_i, fname in tqdm(enumerate(pdf_fnames[:3])):\n",
    "    file_path = os.path.join(pdf_dir, fname)\n",
    "    try:\n",
    "        document = reader.run(\n",
    "            file_path,\n",
    "            extra_info = {\n",
    "                \"source_id\": f\"allganize-RAG-Evaluation-Dataset-KO/finance/{doc_i}\", # arbitrary id\n",
    "                \"domain\": \"finance\",\n",
    "                \"source_file\": fname\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"[READER] failed {} - {}\".format(fname, str(e)))\n",
    "        failed_fnames.append(fname)\n",
    "        continue\n",
    "    document.nodes = document.nodes\n",
    "    documents.append(document)\n",
    "    \n",
    "for node in document.nodes[:3]:\n",
    "    print(type(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_id': 'allganize-RAG-Evaluation-Dataset-KO/finance/2',\n",
       " 'domain': 'finance',\n",
       " 'source_file': '2024년 3월_3. 향후 통화신용정책 방향.pdf'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = document.nodes[0].image\n",
    "\n",
    "# # Crop to half\n",
    "# width, height = image.size\n",
    "# left_half = image.crop((0, 0, width, height//2))\n",
    "# left_half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process Document into Chunks\n",
    "1. merge text nodes with `TextNodeMerger`\n",
    "2. split texts into chunks with `LangchainRecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.processor.document.text_merger import TextNodeMerger\n",
    "# Split Documents page-level\n",
    "merger = TextNodeMerger()\n",
    "\n",
    "merged_documents = []\n",
    "for document in documents:\n",
    "    merged_document = merger.run(document)\n",
    "    merged_documents.append(merged_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='68f777e2-a01b-4e2e-b8bb-f618c8d5c2ef', metadata={'page_no': 1}, text_type=<TextType.PLAIN: 'plain'>, label=<TextLabel.PLAIN: 'plain'>, resource=MediaResource(data=None, text='증권사 리서치센터장, 자산운용사 대표와 함께하는 제1회 증시 콘서트\\n2019 하반기 증시 대전망\\n|\\xa0일\\xa0시\\xa0| 2019.\\xa07.\\xa02\\xa0(화)\\xa014:30\\n|\\xa0장\\xa0소\\xa0| 금융투자협회\\xa03층\\xa0불스홀', path=None, url=None, mimetype=None))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merged_documents[0]\n",
    "merged_documents[0].nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "# 3. Run Splitter\n",
    "from core.splitter.text.langchain_text_splitters import LangchainRecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = LangchainRecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 128\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for document in merged_documents:\n",
    "    document_chunks = []\n",
    "    source_id = document.id_\n",
    "    for i, node in enumerate(document.nodes):\n",
    "        # Run Splitter\n",
    "        if isinstance(node, TextNode):\n",
    "            try:\n",
    "                split_nodes = splitter.run(node)\n",
    "            except Exception as e:\n",
    "                print(i, node)\n",
    "                print(str(e))\n",
    "                raise e\n",
    "        else:\n",
    "            split_nodes = [node]\n",
    "        \n",
    "        # Create New Document\n",
    "        for split_node in split_nodes:\n",
    "            # Each Document contains single node\n",
    "            chunk = Document(\n",
    "                nodes=[split_node],\n",
    "                metadata={\n",
    "                    \"source_id\": source_id,\n",
    "                    \"domain\": document.metadata[\"domain\"],\n",
    "                    \"source_file\": document.metadata['source_file'],\n",
    "                }\n",
    "            )\n",
    "            document_chunks.append(chunk)\n",
    "    chunks.extend(document_chunks)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Embed Using ColPali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/docling/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model\n"
     ]
    }
   ],
   "source": [
    "## Load Model\n",
    "import torch\n",
    "from visual_bge.modeling import Visualized_BGE\n",
    "\n",
    "# Load Colpali engine\n",
    "bge_m3_model_dir = os.path.join(\n",
    "    settings.model_weight_dir, \"bge-m3\"\n",
    ")\n",
    "visualized_model_dir = os.path.join(\n",
    "    settings.model_weight_dir, \"baai-bge-visualized/Visualized_m3.pth\"\n",
    ")\n",
    "\n",
    "model = Visualized_BGE(\n",
    "    model_name_bge = bge_m3_model_dir,\n",
    "    model_weight= visualized_model_dir\n",
    ")\n",
    "model.eval()\n",
    "print(\"Loaded Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.embedder.flagembedding import (\n",
    "    VisualizedBGEInput, \n",
    "    LocalVisualizedBGEEmbedder\n",
    ")\n",
    "embedder = LocalVisualizedBGEEmbedder(\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embedding_input(chunk: Document):\n",
    "    # Single \n",
    "    node = chunk.nodes[0]\n",
    "    if isinstance(node, TextNode):\n",
    "        return VisualizedBGEInput(\n",
    "            text=node.text\n",
    "        )\n",
    "    elif isinstance(node, ImageNode) or isinstance(node, TableNode):\n",
    "        return VisualizedBGEInput(\n",
    "            text=\"{} {}\".format(\n",
    "                node.caption, node.text\n",
    "            ),\n",
    "            image=node.image\n",
    "        )\n",
    "        \n",
    "inputs = [prepare_embedding_input(x) for x in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [05:36<00:00,  9.36s/it]\n",
      "100%|██████████| 31/31 [00:48<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedder.run(inputs, batch_size = 4, disable_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "# (num_chunks, seq_len, embedding_dim)\n",
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Insert into VectorStore\n",
    "* intialize qdrant in-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from core.storage.vectorstore.qdrant import QdrantSingleVectorStore\n",
    "\n",
    "# initialize client\n",
    "client = QdrantClient(\":memory:\")\n",
    "collection_name = \"allganize-finance\"\n",
    "\n",
    "vector_store = QdrantSingleVectorStore(\n",
    "    collection_name=collection_name,\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "vector_store.create_collection(\n",
    "    on_disk_payload=True,  # store the payload on disk\n",
    "    vectors_config = models.VectorParams(\n",
    "        size=embedding_dim,\n",
    "        distance=models.Distance.COSINE,\n",
    "        on_disk=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add(\n",
    "    documents=chunks,\n",
    "    embeddings=embeddings,\n",
    "    metadata_keys=[\"source_file\", \"source_id\", \"title\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6ccd0e26-ef12-43f9-a7b0-56947c9898c8'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = vector_store._client.retrieve(\n",
    "    collection_name=vector_store.collection_name,\n",
    "    ids=[chunks[0].id_],\n",
    "    with_vectors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6ccd0e26-ef12-43f9-a7b0-56947c9898c8\n",
      "{'source_id': 'c1f7a40f-1422-41a0-b443-4729ee0a31c4', 'source_file': '★2019 제1회 증시콘서트 자료집_최종★.pdf'}\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(points[0].id)\n",
    "print(points[0].payload)\n",
    "print(len(points[0].vector))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling",
   "language": "python",
   "name": "docling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
