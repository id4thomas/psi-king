{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# allganize-RAG-Evaluation data + multimodal hybrid search\n",
    "## Methodology\n",
    "1. Read PDF files with `Reader`\n",
    "    * Try `DoclingPDFReader` with `PDF2ImageReader` as fallback\n",
    "2. Chunk `Document` into single-node `Document`\n",
    "3. Embed chunk `Document` instances\n",
    "    * dense: `Visualized_BGE`\n",
    "    * sparse\n",
    "4. Insert into `QdrantSingleHybridVectorStore` vector store\n",
    "5. Test retrieval with queries\n",
    "\n",
    "## Setting\n",
    "* parser:\n",
    "    * IBM [Docling](https://github.com/DS4SD/docling) v2.22.0\n",
    "    * docling-v2 pdf parser backend\n",
    "* dense embedding model: `baai/bge-visualized` (bge-m3 weight)\n",
    "    * https://huggingface.co/BAAI/bge-visualized\n",
    "* data: real-life pdf files from `allganize-RAG-Evaluation-Dataset-KO`\n",
    "    * https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO\n",
    "    * use 10 'finance' domain PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import settings\n",
    "os.environ[\"HF_HOME\"] = settings.docling_model_weight_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "core_src_dir = os.path.join(parent_dir, \"src/psiking\")\n",
    "sys.path.append(core_src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Core Schemas\n",
    "from core.base.schema import Document, TextNode, ImageNode, TableNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read Data\n",
    "* 10 pdf files\n",
    "* try conversion with docling -> use pdf2image as fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Load PDFReaders\n",
    "* (1st) DoclingPDFReader\n",
    "    * use qwen2.5 for picture description & ocr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLM MODEL: Qwen2.5-VL-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from docling.backend.docling_parse_v2_backend import DoclingParseV2DocumentBackend\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    AcceleratorOptions,\n",
    "    PdfPipelineOptions,\n",
    "    PictureDescriptionApiOptions,\n",
    "    TableStructureOptions,\n",
    "    TableFormerMode\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "from core.reader.pdf.docling_reader import DoclingPDFReader\n",
    "\n",
    "from src.docling_vllm_picture_description_pipeline import (\n",
    "    VLLMPictureDescriptionApiOptions,\n",
    "    VLLMPictureDescriptionPdfPipeline\n",
    ")\n",
    "\n",
    "# Initialize format options\n",
    "format_options = PdfPipelineOptions()\n",
    "\n",
    "format_options.accelerator_options = AcceleratorOptions(device=\"mps\")\n",
    "\n",
    "format_options.images_scale = 1.5\n",
    "format_options.generate_page_images = True\n",
    "format_options.generate_picture_images = True\n",
    "format_options.do_ocr = False\n",
    "\n",
    "# Image description\n",
    "print(\"VLM MODEL:\", settings.vlm_model)\n",
    "\n",
    "DESCRIPTION_INSTRUCTION = '''주어진 이미지에대해 2가지 정보를 반환합니다.\n",
    "* description: 최대 2문장 정도로 이미지에 대한 간결한 설명\n",
    "* text: 이미지 내에서 인식된 모든 텍스트\n",
    "다음 JSON 형식으로 반환하세요 {\"description\": str, \"text\": str}'''\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "    description: str\n",
    "    text: str\n",
    "\n",
    "image_description_options = VLLMPictureDescriptionApiOptions(\n",
    "    url=f\"{settings.vlm_base_url}/v1/chat/completions\",\n",
    "    params=dict(\n",
    "        model=settings.vlm_model,\n",
    "        seed=42,\n",
    "        max_completion_tokens=512,\n",
    "        temperature=0.9,\n",
    "        extra_body={\"guided_json\": ImageDescription.model_json_schema()}\n",
    "    ),\n",
    "    # prompt=\"이미지에 대해 최대 2문장 정도로 설명하고 있는 텍스트를 모두 추출하세요. 이미지에 정보가 없다면 설명 텍스트를 작성하지 않습니다. 인식 텍스트와 설명만 반환하세요.\",\n",
    "    prompt=DESCRIPTION_INSTRUCTION,\n",
    "    batch_size=6, # Not implemented inside\n",
    "    scale=0.9,\n",
    "    timeout=90,\n",
    "    min_coverage_area_threshold=0.01,\n",
    "    bitmap_area_threshold=0.1 # 10% of page area\n",
    ")\n",
    "\n",
    "format_options.do_picture_description = True\n",
    "# format_options.do_picture_description = False\n",
    "format_options.enable_remote_services = True\n",
    "format_options.picture_description_options = image_description_options\n",
    "\n",
    "# TableStructure\n",
    "format_options.do_table_structure = True\n",
    "format_options.table_structure_options = TableStructureOptions(mode=TableFormerMode.ACCURATE)\n",
    "\n",
    "# Initialize Converter\n",
    "converter = DocumentConverter(\n",
    "    allowed_formats = [\n",
    "        InputFormat.PDF,\n",
    "    ],\n",
    "    format_options = {\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_cls=VLLMPictureDescriptionPdfPipeline,\n",
    "            pipeline_options = format_options,\n",
    "            backend = DoclingParseV2DocumentBackend\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize reader\n",
    "docling_reader = DoclingPDFReader(converter=converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.reader import PDF2ImageReader\n",
    "\n",
    "# testing on macOS, provide poppler path manually\n",
    "poppler_path = \"/opt/homebrew/Cellar/poppler/25.01.0/bin\"\n",
    "pdf2img_reader = PDF2ImageReader(poppler_path=poppler_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Load PDF fnames, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num files: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['★2019 제1회 증시콘서트 자료집_최종★.pdf',\n",
       " '240409(보도자료) 금융위 핀테크 투자 생태계 활성화 나선다.pdf',\n",
       " '2024년 3월_3. 향후 통화신용정책 방향.pdf',\n",
       " '133178946057443204_WP22-05.pdf',\n",
       " '240130(보도자료) 지방은행의 시중은행 전환시 인가방식 및 절차.pdf',\n",
       " '130292099630937500_KIFVIP2013-10.pdf',\n",
       " '2024년 3월_2. 통화신용정책 운영.pdf',\n",
       " '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf',\n",
       " '240320(보도자료) 금융권의 상생금융 추진현황.pdf',\n",
       " '한-호주 퇴직연금 포럼_책자(최종).pdf']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF File directory\n",
    "pdf_dir = os.path.join(settings.data_dir, \"allganize-RAG-Evaluation-Dataset-KO/finance\")\n",
    "pdf_fnames =[x for x in os.listdir(pdf_dir) if x.endswith(\".pdf\")]\n",
    "print(\"num files:\", len(pdf_fnames))\n",
    "pdf_fnames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★2019 제1회 증시콘서트 자료집_최종★.pdf\n",
      "NUM IMAGES TO ANNOTATE 4\n",
      "NUM IMAGES TO ANNOTATE 11\n",
      "NUM IMAGES TO ANNOTATE 10\n",
      "NUM IMAGES TO ANNOTATE 13\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:29, 89.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240409(보도자료) 금융위 핀테크 투자 생태계 활성화 나선다.pdf\n",
      "NUM IMAGES TO ANNOTATE 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:37, 41.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024년 3월_3. 향후 통화신용정책 방향.pdf\n",
      "NUM IMAGES TO ANNOTATE 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [02:09, 37.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133178946057443204_WP22-05.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered an error during conversion of document 02616dbc4dc47f992b7008e68e4f1d4cb49ccece229e7fad02a38a3470346a63:\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/pipeline/base_pipeline.py\", line 163, in _build_document\n",
      "    for p in pipeline_pages:  # Must exhaust!\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/pipeline/base_pipeline.py\", line 127, in _apply_on_pages\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/models/page_assemble_model.py\", line 68, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/models/table_structure_model.py\", line 178, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/models/layout_model.py\", line 146, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/models/easyocr_model.py\", line 124, in __call__\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/models/page_preprocessing_model.py\", line 25, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/pipeline/standard_pdf_pipeline.py\", line 229, in initialize_page\n",
      "    page._backend = conv_res.input._backend.load_page(page.page_no)  # type: ignore\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/backend/docling_parse_v2_backend.py\", line 239, in load_page\n",
      "    return DoclingParseV2PageBackend(\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/backend/docling_parse_v2_backend.py\", line 27, in __init__\n",
      "    parsed_page = parser.parse_pdf_from_key_on_page(document_hash, page_no)\n",
      "\n",
      "RuntimeError: Invalid code point\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOCLING READER] failed 133178946057443204_WP22-05.pdf - Invalid code point\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [02:24, 28.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240130(보도자료) 지방은행의 시중은행 전환시 인가방식 및 절차.pdf\n",
      "NUM IMAGES TO ANNOTATE 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [02:29, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130292099630937500_KIFVIP2013-10.pdf\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [03:08, 26.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024년 3월_2. 통화신용정책 운영.pdf\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 16\n",
      "NUM IMAGES TO ANNOTATE 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [07:08, 96.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [07:15, 67.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240320(보도자료) 금융권의 상생금융 추진현황.pdf\n",
      "NUM IMAGES TO ANNOTATE 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [07:25, 49.76s/it]Encountered an error during conversion of document ce014774ce984417127bff298a0e883db7ad2652e7cb66d49bbbb2423cc4176c:\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/pipeline/base_pipeline.py\", line 163, in _build_document\n",
      "    for p in pipeline_pages:  # Must exhaust!\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/pipeline/base_pipeline.py\", line 127, in _apply_on_pages\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/models/page_assemble_model.py\", line 68, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/models/table_structure_model.py\", line 178, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/models/layout_model.py\", line 146, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/models/easyocr_model.py\", line 124, in __call__\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/models/page_preprocessing_model.py\", line 25, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/pipeline/standard_pdf_pipeline.py\", line 229, in initialize_page\n",
      "    page._backend = conv_res.input._backend.load_page(page.page_no)  # type: ignore\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/backend/docling_parse_v2_backend.py\", line 239, in load_page\n",
      "    return DoclingParseV2PageBackend(\n",
      "\n",
      "  File \"/opt/miniconda3/envs/docling/lib/python3.10/site-packages/docling/backend/docling_parse_v2_backend.py\", line 27, in __init__\n",
      "    parsed_page = parser.parse_pdf_from_key_on_page(document_hash, page_no)\n",
      "\n",
      "RuntimeError: Invalid code point\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한-호주 퇴직연금 포럼_책자(최종).pdf\n",
      "[DOCLING READER] failed 한-호주 퇴직연금 포럼_책자(최종).pdf - Invalid code point\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [07:32, 45.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'core.base.schema.ImageNode'>\n",
      "<class 'core.base.schema.ImageNode'>\n",
      "<class 'core.base.schema.ImageNode'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert pages to image\n",
    "documents = []\n",
    "docling_failed_fnames = []\n",
    "pdf2img_failed_fnames = []\n",
    "for doc_i, fname in tqdm(enumerate(pdf_fnames)):\n",
    "    file_path = os.path.join(pdf_dir, fname)\n",
    "    print(fname)\n",
    "    extra_info = {\n",
    "        \"source_id\": f\"allganize-RAG-Evaluation-Dataset-KO/finance/{doc_i}\", # arbitrary id\n",
    "        \"domain\": \"finance\",\n",
    "        \"source_file\": fname\n",
    "    }\n",
    "    try:\n",
    "        document = docling_reader.run(\n",
    "            file_path,\n",
    "            extra_info=extra_info\n",
    "        )\n",
    "        documents.append(document)\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(\"[DOCLING READER] failed {} - {}\".format(fname, str(e)))\n",
    "        docling_failed_fnames.append(fname)\n",
    "    \n",
    "    try:\n",
    "        document = pdf2img_reader.run(\n",
    "            file_path,\n",
    "            extra_info=extra_info\n",
    "        )\n",
    "        documents.append(document)\n",
    "    except Exception as e:\n",
    "        print(\"[PDF2IMG READER] failed {} - {}\".format(fname, str(e)))\n",
    "        pdf2img_failed_fnames.append(fname)\n",
    "    \n",
    "for node in document.nodes[:3]:\n",
    "    print(type(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_id': 'allganize-RAG-Evaluation-Dataset-KO/finance/9',\n",
       " 'domain': 'finance',\n",
       " 'source_file': '한-호주 퇴직연금 포럼_책자(최종).pdf'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = document.nodes[0].image\n",
    "\n",
    "# # Crop to half\n",
    "# width, height = image.size\n",
    "# left_half = image.crop((0, 0, width, height//2))\n",
    "# left_half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process Document into Chunks\n",
    "1. merge text nodes with `TextNodeMerger`\n",
    "2. split texts into chunks with `LangchainRecursiveCharacterTextSplitter`\n",
    "3. filter chunks with min length strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.processor.document.text_merger import TextNodeMerger\n",
    "\n",
    "# Split Documents page-level\n",
    "merger = TextNodeMerger()\n",
    "\n",
    "merged_documents = []\n",
    "for document in documents:\n",
    "    merged_document = merger.run(document)\n",
    "    merged_documents.append(merged_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='ee1a443e-e74f-4357-971a-59aff1d8cb69', metadata={'page_no': 1}, text_type=<TextType.PLAIN: 'plain'>, label=<TextLabel.PLAIN: 'plain'>, resource=MediaResource(data=None, text='증권사 리서치센터장, 자산운용사 대표와 함께하는 제1회 증시 콘서트\\n2019 하반기 증시 대전망\\n|\\xa0일\\xa0시\\xa0| 2019.\\xa07.\\xa02\\xa0(화)\\xa014:30\\n|\\xa0장\\xa0소\\xa0| 금융투자협회\\xa03층\\xa0불스홀', path=None, url=None, mimetype=None))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merged_documents[0]\n",
    "merged_documents[0].nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8d33211d-4fa7-4473-a0b0-2052ff164632',\n",
       " 'd8951c63-95d5-4081-bf45-0031edfcbd99',\n",
       " '973114e0-8778-4c31-bd82-88dc98836532',\n",
       " 'f711a915-15e8-4259-b48d-1eb2bfbbe5b6',\n",
       " '18c0864c-fb2d-45b2-a0c8-efe9d27c2ba0',\n",
       " '5b27fe8a-709a-4d33-b139-16fac1eec907',\n",
       " '8db7316a-380b-45b7-afb6-320c8017c7c2',\n",
       " '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6',\n",
       " 'f65e6fd6-d6e0-4311-a2f6-2578d4b10385',\n",
       " '4d2cc420-5775-4ae5-9641-6d12b9b55745']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.id_ for x in merged_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n"
     ]
    }
   ],
   "source": [
    "# 3. Run Splitter\n",
    "from core.splitter.text.langchain_text_splitters import LangchainRecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = LangchainRecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 128\n",
    ")\n",
    "\n",
    "min_text_length = 30\n",
    "chunks = []\n",
    "for document in merged_documents:\n",
    "    document_chunks = []\n",
    "    source_id = document.id_\n",
    "    for i, node in enumerate(document.nodes):\n",
    "        # Run Splitter\n",
    "        if isinstance(node, TextNode):\n",
    "            try:\n",
    "                split_nodes = splitter.run(node)\n",
    "            except Exception as e:\n",
    "                print(i, node)\n",
    "                print(str(e))\n",
    "                raise e\n",
    "        else:\n",
    "            split_nodes = [node]\n",
    "        \n",
    "        # Create New Document\n",
    "        for split_node in split_nodes:\n",
    "            ## Filter TextNodes with short lengths\n",
    "            if isinstance(split_node, TextNode) and len(split_node.text.strip())<min_text_length:\n",
    "                continue\n",
    "            \n",
    "            # Each Document contains single node\n",
    "            chunk = Document(\n",
    "                nodes=[split_node],\n",
    "                \n",
    "                metadata={\n",
    "                    \"source_id\": source_id,\n",
    "                    \"domain\": document.metadata[\"domain\"],\n",
    "                    \"source_file\": document.metadata['source_file'],\n",
    "                }\n",
    "            )\n",
    "            document_chunks.append(chunk)\n",
    "    chunks.extend(document_chunks)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005 1005\n"
     ]
    }
   ],
   "source": [
    "chunk_ids =[x.id_ for x in chunks]\n",
    "print(len(chunk_ids), len(set(chunk_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Embed Using VisualizedBGE + BM42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Text Formatter\n",
    "from core.formatter.document.simple import SimpleTextOnlyFormatter\n",
    "\n",
    "# use default templates\n",
    "text_formatter = SimpleTextOnlyFormatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Dense Embedding VisualizedBGE\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/docling/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dense Embedding Model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Model\n",
    "import torch\n",
    "from visual_bge.modeling import Visualized_BGE\n",
    "\n",
    "# Load Colpali engine\n",
    "bge_m3_model_dir = os.path.join(\n",
    "    settings.model_weight_dir, \"bge-m3\"\n",
    ")\n",
    "visualized_model_dir = os.path.join(\n",
    "    settings.model_weight_dir, \"baai-bge-visualized/Visualized_m3.pth\"\n",
    ")\n",
    "\n",
    "dense_embedding_model = Visualized_BGE(\n",
    "    model_name_bge = bge_m3_model_dir,\n",
    "    model_weight= visualized_model_dir\n",
    ")\n",
    "dense_embedding_model.eval()\n",
    "print(\"Loaded Dense Embedding Model\")\n",
    "dense_embedding_model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.embedder.flagembedding import (\n",
    "    VisualizedBGEInput, \n",
    "    LocalVisualizedBGEEmbedder\n",
    ")\n",
    "dense_embedder = LocalVisualizedBGEEmbedder(\n",
    "    model=dense_embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_visualized_bge_input(text_formatter, chunk: Document):\n",
    "    # Single \n",
    "    formatted_text = text_formatter.run([chunk])[0]\n",
    "    \n",
    "    node = chunk.nodes[0]\n",
    "    if isinstance(node, TextNode):\n",
    "        return VisualizedBGEInput(text=formatted_text)\n",
    "    elif isinstance(node, ImageNode) or isinstance(node, TableNode):\n",
    "        return VisualizedBGEInput(\n",
    "            text=formatted_text,\n",
    "            image=node.image\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown node type error {}\".format(type(node)))\n",
    "    \n",
    "visualized_bge_inputs = [prepare_visualized_bge_input(text_formatter, x) for x in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisualizedBGEInput(text='증권사 리서치센터장, 자산운용사 대표와 함께하는 제1회 증시 콘서트\\n2019 하반기 증시 대전망\\n|\\xa0일\\xa0시\\xa0| 2019.\\xa07.\\xa02\\xa0(화)\\xa014:30\\n|\\xa0장\\xa0소\\xa0| 금융투자협회\\xa03층\\xa0불스홀', image=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualized_bge_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [03:16<00:00,  1.73s/it]\n",
      "100%|██████████| 138/138 [04:01<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "dense_embeddings = dense_embedder.run(visualized_bge_inputs, batch_size = 4, disable_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "# (num_chunks, seq_len, embedding_dim)\n",
    "print(len(dense_embeddings))\n",
    "print(len(dense_embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Sparse Embedding (BM42)\n",
    "* Embed using BM42 Sparse embedder model\n",
    "    * https://huggingface.co/Qdrant/all_miniLM_L6_v2_with_attentions\n",
    "\n",
    "### Loading model from pre-downloaded directory\n",
    "* Load model using 'specific model path'\n",
    "    * specific_model_path (Optional[str], optional): The specific path to the onnx model dir if it should be imported from somewhere else\n",
    "    * download_model method skips download phase (available > v0.5.1 )\n",
    "        * https://github.com/qdrant/fastembed/blob/a931f143ef3543234bc9d8d0c305496c67199972/fastembed/common/model_management.py#L367\n",
    "    * build from source with commit `a931f143ef3543234bc9d8d0c305496c67199972`\n",
    "* cache_dir: cache_dir (str, optional): The path to the cache directory.\n",
    "    Can be set using the `FASTEMBED_CACHE_PATH` env variable.\n",
    "    Defaults to `fastembed_cache` in the system's temp directory.\n",
    "```\n",
    "cd poetry\n",
    "poetry build\n",
    "pip install --force-reinstall fastembed-0.5.1-py3-none-any.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/id4thomas/github/psi-king/examples/fastembed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tokenizer_config.json',\n",
       " 'special_tokens_map.json',\n",
       " 'config.json',\n",
       " 'tokenizer.json',\n",
       " 'README.md',\n",
       " 'vocab.txt',\n",
       " 'model.onnx',\n",
       " '.gitattributes',\n",
       " '.git',\n",
       " 'stopwords.txt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"FASTEMBED_CACHE_PATH\"] = str(os.path.join(os.getcwd(), \"fastembed\"))\n",
    "print(os.environ[\"FASTEMBED_CACHE_PATH\"])\n",
    "sparse_model_dir = os.path.join(settings.model_weight_dir, \"fastembed/sparse/all_miniLM_L6_v2_with_attentions\")\n",
    "os.listdir(sparse_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/id4thomas/models/fastembed/sparse/all_miniLM_L6_v2_with_attentions\n",
      "[SparseEmbedding(values=array([0.30918342]), indices=array([948991206]))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.3091834199811786], [948991206])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load fastembed model\n",
    "from fastembed import SparseTextEmbedding\n",
    "\n",
    "# test specific_model_path function\n",
    "downloaded_dir = SparseTextEmbedding.download_model(\n",
    "    model={},\n",
    "    cache_dir=os.environ[\"FASTEMBED_CACHE_PATH\"],\n",
    "    specific_model_path=sparse_model_dir,\n",
    ")\n",
    "print(downloaded_dir)\n",
    "\n",
    "sparse_model = SparseTextEmbedding(\n",
    "    model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    "    specific_model_path=sparse_model_dir,\n",
    "    cuda=False,\n",
    "    lazy_load=False\n",
    ")\n",
    "\n",
    "test_embeddings = list(sparse_model.embed([\"hi\"]))\n",
    "print(test_embeddings)\n",
    "test_embeddings[0].values.tolist(), test_embeddings[0].indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embedder\n",
    "from core.embedder.fastembed.local_sparse import LocalFastEmbedSparseEmbedder\n",
    "\n",
    "sparse_embedder = LocalFastEmbedSparseEmbedder(\n",
    "    model=sparse_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sparse_input(text_formatter, chunk: Document):\n",
    "    # Single \n",
    "    formatted_text = text_formatter.run([chunk])[0]\n",
    "    return formatted_text\n",
    "\n",
    "sparse_inputs = [prepare_sparse_input(text_formatter, x) for x in chunks]\n",
    "sparse_embedding_values, sparse_embedding_indices = sparse_embedder.run(\n",
    "    sparse_inputs, batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27762097595534047, 0.2596218248069528, 0.29100913226138186, 0.2296326768039164, 0.11464637476009029]\n",
      "[186075762, 777355938, 1724316797, 214838547, 1558169044]\n"
     ]
    }
   ],
   "source": [
    "print(sparse_embedding_values[0][:5])\n",
    "print(sparse_embedding_indices[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x.id_ for x in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n"
     ]
    }
   ],
   "source": [
    "from core.storage.docstore import InMemoryDocumentStore\n",
    "\n",
    "docstore = InMemoryDocumentStore()\n",
    "docstore.add(chunks)\n",
    "print(docstore.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Insert into VectorStore\n",
    "* intialize qdrant in-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from core.storage.vectorstore.qdrant import QdrantSingleHybridVectorStore\n",
    "\n",
    "\n",
    "# initialize client\n",
    "client = QdrantClient(\":memory:\")\n",
    "collection_name = \"allganize-finance\"\n",
    "\n",
    "vector_store = QdrantSingleHybridVectorStore(\n",
    "    collection_name=collection_name,\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Collection\n",
    "from qdrant_client.http import models\n",
    "\n",
    "# bge-m3 1024 dim\n",
    "dense_embedding_dim=1024\n",
    "dense_vectors_config = models.VectorParams(\n",
    "    size=dense_embedding_dim,\n",
    "    distance=models.Distance.COSINE,\n",
    "    on_disk=True,\n",
    "    hnsw_config = {\n",
    "        \"m\": 16,\n",
    "        \"ef_construct\": 100,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sparse BM42 Embedding\n",
    "sparse_vectors_config = models.SparseVectorParams(\n",
    "    modifier=models.Modifier.IDF, ## uses indices from bm42 embedder\n",
    ")\n",
    "\n",
    "# Create VectorStore\n",
    "vector_store.create_collection(\n",
    "    dense_vector_config=dense_vectors_config,\n",
    "    sparse_vector_config=sparse_vectors_config,\n",
    "    on_disk_payload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add(\n",
    "    documents=chunks,\n",
    "    texts=sparse_inputs,\n",
    "    dense_embeddings=dense_embeddings,\n",
    "    sparse_embedding_values=sparse_embedding_values,\n",
    "    sparse_embedding_indices=sparse_embedding_indices,\n",
    "    metadata_keys=[\"source_file\", \"source_id\", \"title\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"status\": \"green\",\n",
      "    \"optimizer_status\": \"ok\",\n",
      "    \"vectors_count\": null,\n",
      "    \"indexed_vectors_count\": 0,\n",
      "    \"points_count\": 1005,\n",
      "    \"segments_count\": 1,\n",
      "    \"config\": {\n",
      "        \"params\": {\n",
      "            \"vectors\": {\n",
      "                \"vector_dense\": {\n",
      "                    \"size\": 1024,\n",
      "                    \"distance\": \"Cosine\",\n",
      "                    \"hnsw_config\": {\n",
      "                        \"m\": 16,\n",
      "                        \"ef_construct\": 100,\n",
      "                        \"full_scan_threshold\": null,\n",
      "                        \"max_indexing_threads\": null,\n",
      "                        \"on_disk\": null,\n",
      "                        \"payload_m\": null\n",
      "                    },\n",
      "                    \"quantization_config\": null,\n",
      "                    \"on_disk\": true,\n",
      "                    \"datatype\": null,\n",
      "                    \"multivector_config\": null\n",
      "                }\n",
      "            },\n",
      "            \"shard_number\": null,\n",
      "            \"sharding_method\": null,\n",
      "            \"replication_factor\": null,\n",
      "            \"write_consistency_factor\": null,\n",
      "            \"read_fan_out_factor\": null,\n",
      "            \"on_disk_payload\": null,\n",
      "            \"sparse_vectors\": {\n",
      "                \"vector_sparse\": {\n",
      "                    \"index\": null,\n",
      "                    \"modifier\": \"idf\"\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"hnsw_config\": {\n",
      "            \"m\": 16,\n",
      "            \"ef_construct\": 100,\n",
      "            \"full_scan_threshold\": 10000,\n",
      "            \"max_indexing_threads\": 0,\n",
      "            \"on_disk\": null,\n",
      "            \"payload_m\": null\n",
      "        },\n",
      "        \"optimizer_config\": {\n",
      "            \"deleted_threshold\": 0.2,\n",
      "            \"vacuum_min_vector_number\": 1000,\n",
      "            \"default_segment_number\": 0,\n",
      "            \"max_segment_size\": null,\n",
      "            \"memmap_threshold\": null,\n",
      "            \"indexing_threshold\": 20000,\n",
      "            \"flush_interval_sec\": 5,\n",
      "            \"max_optimization_threads\": 1\n",
      "        },\n",
      "        \"wal_config\": {\n",
      "            \"wal_capacity_mb\": 32,\n",
      "            \"wal_segments_ahead\": 0\n",
      "        },\n",
      "        \"quantization_config\": null,\n",
      "        \"strict_mode_config\": null\n",
      "    },\n",
      "    \"payload_schema\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# check collection\n",
    "collection_info = vector_store._client.get_collection(\n",
    "    collection_name=vector_store.collection_name\n",
    ")\n",
    "print(collection_info.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'727b33b8-db08-4632-874d-ff414c20d3e5'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = vector_store._client.retrieve(\n",
    "    collection_name=vector_store.collection_name,\n",
    "    ids=[chunks[0].id_],\n",
    "    with_vectors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727b33b8-db08-4632-874d-ff414c20d3e5\n",
      "{'source_id': '8d33211d-4fa7-4473-a0b0-2052ff164632', 'source_file': '★2019 제1회 증시콘서트 자료집_최종★.pdf'}\n",
      "1024\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(points[0].id)\n",
    "print(points[0].payload)\n",
    "\n",
    "# Dense Vector\n",
    "print(len(points[0].vector['vector_dense']))\n",
    "\n",
    "# Sparse Vector\n",
    "print(len(points[0].vector['vector_sparse'].indices))\n",
    "print(len(points[0].vector['vector_sparse'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test Retrieval with Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "8 [0.31059375711328135, 0.31304877079908167, 0.19882314306607887, 0.1964898348134671, 0.32203981694197703, 0.3009219191747245, 0.10172041730715874, 0.33753322982893214]\n",
      "8 [1024444394, 1285937098, 693871510, 376689346, 332251539, 1798584096, 1061271926, 1903036828]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Embed Query\n",
    "query = \"시중은행, 지방은행, 인터넷은행의 인가 요건 및 절차에 차이가 있는데 그 차이점은 무엇인가요?\"\n",
    "\n",
    "query_dense_embedding = dense_embedder.run(\n",
    "    [VisualizedBGEInput(text=query)],\n",
    "    batch_size = 4,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "query_sparse_embedding_values, query_sparse_embedding_indices = sparse_embedder.run(\n",
    "    [query], batch_size = 1\n",
    ")\n",
    "\n",
    "print(len(query_dense_embedding[0]))\n",
    "print(len(query_sparse_embedding_values[0]), query_sparse_embedding_values[0])\n",
    "print(len(query_sparse_embedding_indices[0]), query_sparse_embedding_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Query\n",
    "results = vector_store.query(\n",
    "    mode=\"hybrid\",\n",
    "    dense_embedding=query_dense_embedding[0],\n",
    "    sparse_embedding_values=query_sparse_embedding_values[0],\n",
    "    sparse_embedding_indices=query_sparse_embedding_indices[0],\n",
    "    limit=10\n",
    ")\n",
    "print(len(results.points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='89e55099-0a9d-42e2-80a1-6bf84df97030' version=0 score=0.8333333333333333 payload={'source_id': '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6', 'source_file': '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'} vector=None shard_key=None order_value=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source_id': '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6',\n",
       " 'source_file': '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(results.points[0])\n",
    "results.points[0].payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89e55099-0a9d-42e2-80a1-6bf84df97030 - score 0.833\n",
      "{'source_id': '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6', 'domain': 'finance', 'source_file': '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'- *  (은행법  §8 ➀ )  은행업을 경영하려는 자는 금융위원회의 인가를 받아야 한다.\\n\\t- ㅇ 시중은행전국영업뿐만 아니라 지방은행 및 인터넷은행도 모두 ( ) 동일한 조항제'\n",
      "------------------------------\n",
      "7ae4620f-88eb-4325-bf77-95e3a54dea10 - score 0.600\n",
      "{'source_id': '18c0864c-fb2d-45b2-a0c8-efe9d27c2ba0', 'domain': 'finance', 'source_file': '240130(보도자료) 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'금융위원회\\n보도자료\\n보도시점\\n20 2 4 . 1 . 3 1 . ( 수  금\\n)\\n융위  회 의   후\\n(별도공지)\\n배포\\n2024.1.30.(화) 10:00\\n지방은행의 시중은행 전환시'\n",
      "------------------------------\n",
      "bf9b22bd-930e-4864-9aa8-7ab4dd3d3edb - score 0.476\n",
      "{'source_id': '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6', 'domain': 'finance', 'source_file': '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'Ⅱ .  지방은행의 시중은행 전환시 인가방식 및 절차\\n1.  인가  방식\\n가. 쟁점 사항 : ➊ 신규인가 vs ➋ 기존 인가내용을 변경\\n- □ 은행법 제 조의 인가규정은 신규인가 '\n",
      "------------------------------\n",
      "32615c99-5c48-47fe-865b-5f5d4f6e2bf1 - score 0.417\n",
      "{'source_id': '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6', 'domain': 'finance', 'source_file': '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'- ㅇ 지방은행 인가에 대한 별도 폐업인가가 불필요하며 이에 따라 , 기존 법률관계의 승계여부에 대한 문제도 발생하지 않음\\n2.  심사  내용\\n가. 쟁점 사항 : 모든 세부심사요건'\n",
      "------------------------------\n",
      "5ab3b176-9f67-405c-abdb-1da7e9d15a1f - score 0.325\n",
      "{'source_id': '18c0864c-fb2d-45b2-a0c8-efe9d27c2ba0', 'domain': 'finance', 'source_file': '240130(보도자료) 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'다만,  현행  은행법령상 지방은행의 시중은행 전환에 관한 명시적인 규정은 없으며,  종전에도  은행  종류의  전환  사례는  없었습니다.  지방은행의  정관 에서  특정지역으로'\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for point in results.points[:5]:\n",
    "    point_id = point.id\n",
    "    point_chunk = docstore.get([point_id])[0]\n",
    "    print(\"{} - score {:.3f}\".format(point_id, point.score))\n",
    "    print(point_chunk.metadata)\n",
    "    print(type(point_chunk.nodes[0]))\n",
    "    print(repr(point_chunk.nodes[0].text[:100]))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Dense-Only Query\n",
    "results = vector_store.query(\n",
    "    mode=\"dense\",\n",
    "    dense_embedding=query_dense_embedding[0],\n",
    "    limit=100\n",
    ")\n",
    "print(len(results.points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7ae4620f-88eb-4325-bf77-95e3a54dea10 - score 0.773\n",
      "{'source_id': '18c0864c-fb2d-45b2-a0c8-efe9d27c2ba0', 'domain': 'finance', 'source_file': '240130(보도자료) 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'금융위원회\\n보도자료\\n보도시점\\n20 2 4 . 1 . 3 1 . ( 수  금\\n)\\n융위  회 의   후\\n(별도공지)\\n배포\\n2024.1.30.(화) 10:00\\n지방은행의 시중은행 전환시'\n",
      "------------------------------\n",
      "89e55099-0a9d-42e2-80a1-6bf84df97030 - score 0.761\n",
      "{'source_id': '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6', 'domain': 'finance', 'source_file': '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'- *  (은행법  §8 ➀ )  은행업을 경영하려는 자는 금융위원회의 인가를 받아야 한다.\\n\\t- ㅇ 시중은행전국영업뿐만 아니라 지방은행 및 인터넷은행도 모두 ( ) 동일한 조항제'\n",
      "------------------------------\n",
      "32615c99-5c48-47fe-865b-5f5d4f6e2bf1 - score 0.731\n",
      "{'source_id': '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6', 'domain': 'finance', 'source_file': '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'- ㅇ 지방은행 인가에 대한 별도 폐업인가가 불필요하며 이에 따라 , 기존 법률관계의 승계여부에 대한 문제도 발생하지 않음\\n2.  심사  내용\\n가. 쟁점 사항 : 모든 세부심사요건'\n",
      "------------------------------\n",
      "5ab3b176-9f67-405c-abdb-1da7e9d15a1f - score 0.731\n",
      "{'source_id': '18c0864c-fb2d-45b2-a0c8-efe9d27c2ba0', 'domain': 'finance', 'source_file': '240130(보도자료) 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'다만,  현행  은행법령상 지방은행의 시중은행 전환에 관한 명시적인 규정은 없으며,  종전에도  은행  종류의  전환  사례는  없었습니다.  지방은행의  정관 에서  특정지역으로'\n",
      "------------------------------\n",
      "6e7ac1db-8037-4b5c-a450-41292d260b27 - score 0.714\n",
      "{'source_id': '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6', 'domain': 'finance', 'source_file': '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "\"Ⅰ .  검토  배경\\n- □ 정부는 은행권 경쟁촉진을 위해 지방은행의 시중은행 전환을 허용하겠다고 발표 * ('23.7.5 일 )\\n- *  '금융당국,  은행업에 공정하고 실효성 \"\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for point in results.points[:5]:\n",
    "    point_id = point.id\n",
    "    point_chunk = docstore.get([point_id])[0]\n",
    "    print(\"{} - score {:.3f}\".format(point_id, point.score))\n",
    "    print(point_chunk.metadata)\n",
    "    print(type(point_chunk.nodes[0]))\n",
    "    print(repr(point_chunk.nodes[0].text[:100]))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# Sparse-Only Query\n",
    "results = vector_store.query(\n",
    "    mode=\"sparse\",\n",
    "    sparse_embedding_values=query_sparse_embedding_values[0],\n",
    "    sparse_embedding_indices=query_sparse_embedding_indices[0],\n",
    "    limit=100\n",
    ")\n",
    "print(len(results.points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89e55099-0a9d-42e2-80a1-6bf84df97030 - score 0.646\n",
      "{'source_id': '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6', 'domain': 'finance', 'source_file': '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'- *  (은행법  §8 ➀ )  은행업을 경영하려는 자는 금융위원회의 인가를 받아야 한다.\\n\\t- ㅇ 시중은행전국영업뿐만 아니라 지방은행 및 인터넷은행도 모두 ( ) 동일한 조항제'\n",
      "------------------------------\n",
      "bf9b22bd-930e-4864-9aa8-7ab4dd3d3edb - score 0.513\n",
      "{'source_id': '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6', 'domain': 'finance', 'source_file': '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'Ⅱ .  지방은행의 시중은행 전환시 인가방식 및 절차\\n1.  인가  방식\\n가. 쟁점 사항 : ➊ 신규인가 vs ➋ 기존 인가내용을 변경\\n- □ 은행법 제 조의 인가규정은 신규인가 '\n",
      "------------------------------\n",
      "85ef73c5-f9f5-4fe5-b098-1d79aaa2f1a7 - score 0.489\n",
      "{'source_id': '8db7316a-380b-45b7-afb6-320c8017c7c2', 'domain': 'finance', 'source_file': '2024년 3월_2. 통화신용정책 운영.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'③ 2023년 미 SVB 사태와의 차이점\\n2023년 미 SVB 사태와 미 CRE발 리스크의 가장 큰 차이점은 전자의 경우 중소 지역은행들에 대한 예 금인출 사태(bank-run)로'\n",
      "------------------------------\n",
      "fe542f23-e96e-45d5-9ad5-9585c0b59ef1 - score 0.449\n",
      "{'source_id': '8db7316a-380b-45b7-afb6-320c8017c7c2', 'domain': 'finance', 'source_file': '2024년 3월_2. 통화신용정책 운영.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'미 은행의 대출자산 중 CRE 대출 2) 이 차지하는 비 중은 은행 규모에 따라 큰 차이가 있다. 2023년 6월 말 현재 자산규모가 1천억 달러 이상인 은행의 경 우  전체  대'\n",
      "------------------------------\n",
      "32615c99-5c48-47fe-865b-5f5d4f6e2bf1 - score 0.420\n",
      "{'source_id': '4b7e28e5-2c62-4de5-a013-9a2f3525d2f6', 'domain': 'finance', 'source_file': '[별첨] 지방은행의 시중은행 전환시 인가방식 및 절차.pdf'}\n",
      "<class 'core.base.schema.TextNode'>\n",
      "'- ㅇ 지방은행 인가에 대한 별도 폐업인가가 불필요하며 이에 따라 , 기존 법률관계의 승계여부에 대한 문제도 발생하지 않음\\n2.  심사  내용\\n가. 쟁점 사항 : 모든 세부심사요건'\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for point in results.points[:5]:\n",
    "    point_id = point.id\n",
    "    point_chunk = docstore.get([point_id])[0]\n",
    "    print(\"{} - score {:.3f}\".format(point_id, point.score))\n",
    "    print(point_chunk.metadata)\n",
    "    print(type(point_chunk.nodes[0]))\n",
    "    print(repr(point_chunk.nodes[0].text[:100]))\n",
    "    print('-'*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling",
   "language": "python",
   "name": "docling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
