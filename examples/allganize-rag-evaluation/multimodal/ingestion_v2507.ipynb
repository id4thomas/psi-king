{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# allganize-RAG-Evaluation data + multimodal ingestion\n",
    "## Methodology\n",
    "```\n",
    "1. Load Document Readers\n",
    "    1-1. Load DoclingPDFReader\n",
    "        1-1-1. Initialize Docling Converter\n",
    "        1-1-2. Initialize PSIKing Reader\n",
    "    1-2. Load PDF2ImageReader\n",
    "2. Load PDF File Data\n",
    "3. Ingest Data\n",
    "    3-1. (Reader) PDF File -> PSIKing Document\n",
    "    3-2. (Splitter) Chunk Documents\n",
    "4. Embed\n",
    "5. Insert into DocumentStore, VectorStore\n",
    "    5-1. Insert to DocStore\n",
    "    5-2. Insert to VectorStore\n",
    "6. Test Query\n",
    "```\n",
    "\n",
    "## Settings\n",
    "[Dataset]\n",
    "* real-life Korean finance pdf files from `allganize-RAG-Evaluation-Dataset-KO`\n",
    "    * https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO\n",
    "    * use 10 'finance' domain files\n",
    "\n",
    "[Embedder]\n",
    "* model: `jina-embeddings-v4-vllm-retrieval` [[hf link]](https://huggingface.co/jinaai/jina-embeddings-v4-vllm-retrieval)\n",
    "    * served using vLLM `v0.9.1` docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import settings\n",
    "# Artifacts should contain model weights downloaded using `docling-tools models download`\n",
    "# Typically set to `~/.cache/docling/models`\n",
    "# os.environ[\"DOCLING_ARTIFACTS_PATH\"] = settings.docling_artifacts_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Core Schemas\n",
    "from psiking.core.base.schema import Document, TextNode, ImageNode, TableNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Document Readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Load DoclingPDFReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1-1. Initialize Docling Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.types.doc import PictureItem\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    AcceleratorDevice,\n",
    "    VlmPipelineOptions,\n",
    "    PdfPipelineOptions,\n",
    "    PictureDescriptionApiOptions,\n",
    "    ResponseFormat,\n",
    "    TableStructureOptions,\n",
    "    TableFormerMode\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "from docling.datamodel.pipeline_options_vlm_model import (\n",
    "    ApiVlmOptions,\n",
    "    InferenceFramework,\n",
    "    InlineVlmOptions,\n",
    "    ResponseFormat,\n",
    "    TransformersModelType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_options = PdfPipelineOptions()\n",
    "\n",
    "# If force_backend_text = True, text from backend will be used instead of generated text\n",
    "pipeline_options.force_backend_text = False\n",
    "pipeline_options.generate_picture_images = True\n",
    "\n",
    "pipeline_options.images_scale = 1.5\n",
    "pipeline_options.generate_page_images = True\n",
    "pipeline_options.generate_picture_images = True\n",
    "pipeline_options.do_ocr = False\n",
    "\n",
    "# TableStructure\n",
    "pipeline_options.do_table_structure = True\n",
    "pipeline_options.table_structure_options = TableStructureOptions(mode=TableFormerMode.ACCURATE)\n",
    "\n",
    "pipeline_options.accelerator_options.device = AcceleratorDevice.MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.1-nano\n"
     ]
    }
   ],
   "source": [
    "from psiking.core.reader.pdf.docling.picture_description import (\n",
    "    openai_options as docling_openai_picture_description_options\n",
    ") \n",
    "\n",
    "pipeline_options.do_picture_description = True\n",
    "pipeline_options.enable_remote_services = True\n",
    "\n",
    "print(settings.vlm_model)\n",
    "pipeline_options.picture_description_options=docling_openai_picture_description_options(\n",
    "    api_key=settings.vlm_api_key,\n",
    "    model=settings.vlm_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = DocumentConverter(\n",
    "    allowed_formats = [\n",
    "        InputFormat.PDF,\n",
    "    ],\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1-2. Initialize PSIKing Reader (DoclingPDFReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psiking.core.reader.pdf.docling import DoclingPDFReader\n",
    "\n",
    "# initalize reader\n",
    "reader = DoclingPDFReader(converter=converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Load PDF2ImageReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psiking.core.reader.pdf.pdf2image import PDF2ImageReader\n",
    "\n",
    "poppler_path = \"/opt/homebrew/Cellar/poppler/25.07.0/bin\"\n",
    "pdf2img_reader = PDF2ImageReader(poppler_path=poppler_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load PDF File Data\n",
    "* 10 pdf files, convert to image with pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num files: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['7373884a-8255-482d-9e7c-00b919083526.pdf',\n",
       " '5484364a-38de-48b7-a0a6-b009f361bd9e.pdf',\n",
       " 'b59c836c-ec57-44ba-b4a8-2ae3d58a22e4.pdf',\n",
       " '99d45724-817a-4c05-85e2-83e0aa8ac8c0.pdf',\n",
       " '03d95093-ed1f-4a66-83dc-5534dfbd87e3.pdf',\n",
       " 'c94f675e-7d81-48bd-88f8-c5ff766190cc.pdf',\n",
       " '053248f8-4311-413e-b34b-9a65a4251f4f.pdf',\n",
       " '72b54f4b-7002-48ea-ad20-2c613d8360f6.pdf',\n",
       " 'bbd035d6-51a2-41ba-b913-8357d89b7852.pdf',\n",
       " '980889bb-16cd-447f-b5eb-1384b84903cc.pdf']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF File directory\n",
    "# pdf_dir = os.path.join(settings.data_dir, \"retrieval_dataset/allganize-RAG-Evaluation-Dataset-KO/finance\")\n",
    "pdf_dir = '../data/pdf/finance'\n",
    "pdf_fnames =[x for x in os.listdir(pdf_dir) if x.endswith(\".pdf\")]\n",
    "print(\"num files:\", len(pdf_fnames))\n",
    "pdf_fnames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to FileIds\n",
    "metadata_df = pd.read_csv('../data/metadata.tsv', sep='\\t')\n",
    "\n",
    "pdf_file_ids = [\n",
    "    metadata_df[metadata_df.id==x.replace('.pdf', '')].iloc[0]['id'] for x in pdf_fnames\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_file_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. (Reader) PDF File -> PSIKing Document\n",
    "[Note]\n",
    "* Some files may fail to parse with error `RuntimeError: Invalid code point` due to error with docling PDF pase backend\n",
    "    * Related Issues:\n",
    "        * https://github.com/docling-project/docling/issues/1111\n",
    "        * https://github.com/docling-project/docling-parse/issues/133\n",
    "* Files exhibiting this error will be handled as PDF2Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:22<15:32, 116.62s/it]Encountered an error during conversion of document 02616dbc4dc47f992b7008e68e4f1d4cb49ccece229e7fad02a38a3470346a63:\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/pipeline/base_pipeline.py\", line 160, in _build_document\n",
      "    for p in pipeline_pages:  # Must exhaust!\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/pipeline/base_pipeline.py\", line 126, in _apply_on_pages\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/models/page_assemble_model.py\", line 70, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/models/table_structure_model.py\", line 177, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/models/layout_model.py\", line 151, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/models/easyocr_model.py\", line 130, in __call__\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/models/page_preprocessing_model.py\", line 37, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/pipeline/standard_pdf_pipeline.py\", line 177, in initialize_page\n",
      "    page._backend = conv_res.input._backend.load_page(page.page_no)  # type: ignore\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/backend/docling_parse_v4_backend.py\", line 160, in load_page\n",
      "    seg_page = self.dp_doc.get_page(\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling_parse/pdf_parser.py\", line 124, in get_page\n",
      "    doc_dict = self._parser.parse_pdf_from_key_on_page(\n",
      "\n",
      "RuntimeError: Invalid code point\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOCLING READER] failed b59c836c-ec57-44ba-b4a8-2ae3d58a22e4.pdf, Falling back to PDF2IMG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [04:48<04:38, 46.47s/it] Encountered an error during conversion of document ce014774ce984417127bff298a0e883db7ad2652e7cb66d49bbbb2423cc4176c:\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/pipeline/base_pipeline.py\", line 160, in _build_document\n",
      "    for p in pipeline_pages:  # Must exhaust!\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/pipeline/base_pipeline.py\", line 126, in _apply_on_pages\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/models/page_assemble_model.py\", line 70, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/models/table_structure_model.py\", line 177, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/models/layout_model.py\", line 151, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/models/easyocr_model.py\", line 130, in __call__\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/models/page_preprocessing_model.py\", line 37, in __call__\n",
      "    for page in page_batch:\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/pipeline/standard_pdf_pipeline.py\", line 177, in initialize_page\n",
      "    page._backend = conv_res.input._backend.load_page(page.page_no)  # type: ignore\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling/backend/docling_parse_v4_backend.py\", line 160, in load_page\n",
      "    seg_page = self.dp_doc.get_page(\n",
      "\n",
      "  File \"/opt/miniconda3/envs/psiking/lib/python3.10/site-packages/docling_parse/pdf_parser.py\", line 124, in get_page\n",
      "    doc_dict = self._parser.parse_pdf_from_key_on_page(\n",
      "\n",
      "RuntimeError: Invalid code point\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOCLING READER] failed 03d95093-ed1f-4a66-83dc-5534dfbd87e3.pdf, Falling back to PDF2IMG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:49<00:00, 70.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'psiking.core.base.schema.TextNode'>\n",
      "<class 'psiking.core.base.schema.TextNode'>\n",
      "<class 'psiking.core.base.schema.TextNode'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert pages to image\n",
    "documents = []\n",
    "failed_fnames = []\n",
    "\n",
    "for doc_i in tqdm(range(len(pdf_fnames))):\n",
    "    fname=pdf_fnames[doc_i]\n",
    "    file_path = os.path.join(pdf_dir, fname)\n",
    "    file_id = pdf_file_ids[doc_i]\n",
    "    \n",
    "    try:\n",
    "        document = reader.run(\n",
    "            file_path, \n",
    "            extra_info = {\n",
    "                \"source_id\": file_id,\n",
    "                \"domain\": \"finance\"\n",
    "            }\n",
    "        )\n",
    "        documents.append(document)\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        # print(\"[DOCLING READER] failed {} - {}\".format(fname, str(e)))\n",
    "        print(\"[DOCLING READER] failed {}, Falling back to PDF2IMG\".format(fname))\n",
    "        # print(traceback.format_exc())\n",
    "\n",
    "    # Fallback - PDF2IMG\n",
    "    try:\n",
    "        document = pdf2img_reader.run(\n",
    "            file_path,\n",
    "            extra_info = {\n",
    "                \"source_id\": file_id,\n",
    "                \"domain\": \"finance\"\n",
    "            }\n",
    "        )\n",
    "        documents.append(document)\n",
    "    except Exception as e:\n",
    "        print(\"[PDF2IMG READER] failed {} - {}\".format(fname, str(e)))\n",
    "        failed_fnames.append(fname)\n",
    "    \n",
    "for node in document.nodes[:3]:\n",
    "    print(type(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reader': 'DoclingPDFReader',\n",
       " 'source_id': '980889bb-16cd-447f-b5eb-1384b84903cc',\n",
       " 'domain': 'finance'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = document.nodes[0].image\n",
    "\n",
    "# # Crop to half\n",
    "# width, height = image.size\n",
    "# left_half = image.crop((0, 0, width, height//2))\n",
    "# left_half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. (Splitter) Chunk Documents\n",
    "1. merge text nodes with `TextNodeMerger`\n",
    "2. split texts into chunks with `LangchainRecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psiking.core.processor.document.text_merger import TextNodeMerger\n",
    "# Split Documents page-level\n",
    "merger = TextNodeMerger()\n",
    "\n",
    "merged_documents = []\n",
    "for document in documents:\n",
    "    merged_document = merger.run(document)\n",
    "    merged_documents.append(merged_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='aac25e2f-9b7a-487f-831f-c94621894b44', metadata={'prov': '[{\"page_no\": 1, \"bbox\": {\"l\": 71.444, \"t\": 702.6370374023437, \"r\": 511.598, \"b\": 645.7080374023437, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 37]}]'}, text_type=<TextType.PLAIN: 'plain'>, label=<TextLabel.PLAIN: 'plain'>, resource=MediaResource(data=None, text='증권사 리서치센터장, 자산운용사 대표와 함께하는 제1회 증시 콘서트\\n2019 하반기 증시 대전망\\n|\\xa0일\\xa0시\\xa0| 2019.\\xa07.\\xa02\\xa0(화)\\xa014:30\\n|\\xa0장\\xa0소\\xa0| 금융투자협회\\xa03층\\xa0불스홀', path=None, url=None, mimetype=None))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merged_documents[0]\n",
    "merged_documents[0].nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032\n"
     ]
    }
   ],
   "source": [
    "# Run Splitter\n",
    "import copy\n",
    "from psiking.core.splitter.text.langchain_text_splitters import LangchainRecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = LangchainRecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 128\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for document in merged_documents:\n",
    "    document_chunks = []\n",
    "    document_metadata = document.metadata\n",
    "    \n",
    "    for i, node in enumerate(document.nodes):\n",
    "        # Run Splitter\n",
    "        if isinstance(node, TextNode):\n",
    "            try:\n",
    "                split_nodes = splitter.run(node)\n",
    "            except Exception as e:\n",
    "                print(i, node)\n",
    "                print(str(e))\n",
    "                raise e\n",
    "        else:\n",
    "            split_nodes = [node]\n",
    "            \n",
    "        node_metadata = node.metadata\n",
    "        # Add \n",
    "        chunk_metadata = copy.deepcopy(document_metadata)\n",
    "        chunk_metadata['prov'] = node_metadata['prov']\n",
    "        \n",
    "        # Create New Document\n",
    "        for split_node in split_nodes:\n",
    "            # Each Document contains single node\n",
    "            chunk = Document(\n",
    "                nodes=[split_node],\n",
    "                metadata=chunk_metadata\n",
    "            )\n",
    "            document_chunks.append(chunk)\n",
    "    chunks.extend(document_chunks)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1. Initialize Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "from psiking.core.embedder.vllm.online_jina_emb_v4 import VLLMOnlineJinaEmbV4Embedder\n",
    "\n",
    "embedder = VLLMOnlineJinaEmbV4Embedder(\n",
    "    base_url=settings.multimodal_embedding_base_url,\n",
    "    model=settings.multimodal_embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2. Embed Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def img_to_base64(img: Image.Image, format=\"PNG\"):\n",
    "    buffer = BytesIO()\n",
    "    img.save(buffer, format=format)         # or format=\"JPEG\"\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # 3. Base64-encode\n",
    "    b64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    return b64\n",
    "\n",
    "async def embed(semaphore, doc: Document):\n",
    "    node = doc.nodes[0]\n",
    "    \n",
    "    text = doc.nodes[0].text\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'text', 'text': text},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    if isinstance(node, ImageNode) or isinstance(node, TableNode):\n",
    "        if not node.image_data is None:\n",
    "            image_base64 = img_to_base64(node.image)\n",
    "            messages[0]['content'].append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}\n",
    "                }\n",
    "            )\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            embedding = await embedder.arun(\n",
    "                input=messages,\n",
    "                input_format='messages',\n",
    "                pool=True,\n",
    "                normalize=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"ERR DOC {} {}\".format(doc.id_, str(e)))\n",
    "            raise e\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1032/1032 [1:08:40<00:00,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "semaphore = asyncio.Semaphore(16)\n",
    "\n",
    "tasks = []\n",
    "for chunk in chunks:\n",
    "    task = embed(semaphore, chunk)\n",
    "    tasks.append(task)\n",
    "\n",
    "embeddings = await tqdm.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [x.tolist() for x in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "# (num_chunks, seq_len, embedding_dim)\n",
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Insert into DocumentStore, VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1. Insert to DocStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psiking.core.storage.docstore.in_memory import InMemoryDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_store.add(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1032"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_store.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_store.save('storage/docstore_v2507.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-2. Insert to VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from psiking.core.storage.vectorstore.qdrant import QdrantSingleVectorStore\n",
    "\n",
    "# initialize client\n",
    "# client = QdrantClient(\":memory:\")\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "collection_name = \"allganize-finance-multimodal-v2507\"\n",
    "\n",
    "vector_store = QdrantSingleVectorStore(\n",
    "    collection_name=collection_name,\n",
    "    client=client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models\n",
    "\n",
    "# embedding_dim = 1024\n",
    "embedding_dim = len(embeddings[0])\n",
    "\n",
    "vector_store.create_collection(\n",
    "    on_disk_payload=True,  # store the payload on disk\n",
    "    vectors_config = models.VectorParams(\n",
    "        size=embedding_dim,\n",
    "        distance=models.Distance.COSINE,\n",
    "        on_disk=True,\n",
    "        hnsw_config = {\n",
    "            \"m\": 16,\n",
    "            \"ef_construct\": 100,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1032, 1032)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add(\n",
    "    documents=chunks,\n",
    "    embeddings=embeddings,\n",
    "    metadata_keys=[\"source_id\", \"domain\", 'prov']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8bd9aa6b-06cf-4ef8-a617-41d7eec2ab89'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = vector_store._client.retrieve(\n",
    "    collection_name=vector_store.collection_name,\n",
    "    ids=[chunks[0].id_],\n",
    "    with_vectors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8bd9aa6b-06cf-4ef8-a617-41d7eec2ab89\n",
      "{'source_id': '7373884a-8255-482d-9e7c-00b919083526', 'domain': 'finance', 'prov': '[{\"page_no\": 1, \"bbox\": {\"l\": 71.444, \"t\": 702.6370374023437, \"r\": 511.598, \"b\": 645.7080374023437, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 37]}]'}\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(points[0].id)\n",
    "print(points[0].payload)\n",
    "print(len(points[0].vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from psiking.core.storage.vectorstore.schema import (\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    "    VectorStoreQuery,\n",
    "    VectorStoreQueryMode,\n",
    "    VectorStoreQueryOptions,\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random query embedding\n",
    "query_embedding = np.random.randn(embedding_dim)\n",
    "\n",
    "vsquery=VectorStoreQuery(\n",
    "    dense_embedding=query_embedding\n",
    ")\n",
    "vsoptions=VectorStoreQueryOptions(\n",
    "    mode=VectorStoreQueryMode.DENSE,\n",
    "    top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = vector_store.query(\n",
    "    query=vsquery,\n",
    "    options=vsoptions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='f83d0deb-a3c4-4a89-9bdf-f1197ed971aa', version=13, score=0.056224834, payload={'source_id': '980889bb-16cd-447f-b5eb-1384b84903cc', 'domain': 'finance', 'prov': '[{\"page_no\": 57, \"bbox\": {\"l\": 494.632, \"t\": 580.8330073242187, \"r\": 501.125, \"b\": 498.1310073242187, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 21]}]'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='a805d829-95dc-4b88-a22d-76d453f9163b', version=16, score=0.048955273, payload={'source_id': '980889bb-16cd-447f-b5eb-1384b84903cc', 'domain': 'finance', 'prov': '[{\"page_no\": 134, \"bbox\": {\"l\": 430.1534118652344, \"t\": 105.8568115234375, \"r\": 477.24481201171875, \"b\": 58.15570068359375, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 0]}]'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='b90681d6-2aa5-4d0d-bf3e-f05343809908', version=9, score=0.048723314, payload={'source_id': '72b54f4b-7002-48ea-ad20-2c613d8360f6', 'domain': 'finance', 'prov': '[{\"page_no\": 35, \"bbox\": {\"l\": 76.535, \"t\": 332.6149956054687, \"r\": 217.662, \"b\": 327.4829956054687, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 31]}]'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='1fd155f2-a926-45e5-ae85-be3576c03131', version=9, score=0.042463098, payload={'source_id': '72b54f4b-7002-48ea-ad20-2c613d8360f6', 'domain': 'finance', 'prov': '[{\"page_no\": 55, \"bbox\": {\"l\": 362.7659912109375, \"t\": 116.20635986328125, \"r\": 427.7737731933594, \"b\": 66.23095703125, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 0]}]'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='f62c302c-5c6c-4233-abb5-98f0a21be6bb', version=10, score=0.041747257, payload={'source_id': '980889bb-16cd-447f-b5eb-1384b84903cc', 'domain': 'finance', 'prov': '[{\"page_no\": 2, \"bbox\": {\"l\": 175.29417419433594, \"t\": 40.392333984375, \"r\": 357.8405456542969, \"b\": 22.9173583984375, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 0]}]'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='fc39ad71-61bd-4bbe-b2e7-47e46886a0f9', version=9, score=0.040213868, payload={'source_id': '72b54f4b-7002-48ea-ad20-2c613d8360f6', 'domain': 'finance', 'prov': '[{\"page_no\": 36, \"bbox\": {\"l\": 79.72828674316406, \"t\": 578.0823211669922, \"r\": 448.5262756347656, \"b\": 416.5176696777344, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 0]}]'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='5e210a51-e332-4f17-a7d0-c95efe2bf44b', version=9, score=0.03975575, payload={'source_id': '72b54f4b-7002-48ea-ad20-2c613d8360f6', 'domain': 'finance', 'prov': '[{\"page_no\": 20, \"bbox\": {\"l\": 80.06078338623047, \"t\": 597.9434661865234, \"r\": 443.966064453125, \"b\": 409.26910400390625, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 0]}]'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='ab32a1ac-0ad2-48a8-8d61-51b52d774088', version=9, score=0.036903515, payload={'source_id': '72b54f4b-7002-48ea-ad20-2c613d8360f6', 'domain': 'finance', 'prov': '[{\"page_no\": 50, \"bbox\": {\"l\": 77.61466217041016, \"t\": 608.7437286376953, \"r\": 450.5911560058594, \"b\": 98.5546875, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 0]}]'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='a64f487e-9f65-4db6-b429-f716175e3d5a', version=9, score=0.03662803, payload={'source_id': '72b54f4b-7002-48ea-ad20-2c613d8360f6', 'domain': 'finance', 'prov': '[{\"page_no\": 40, \"bbox\": {\"l\": 79.373, \"t\": 388.8009956054687, \"r\": 230.993, \"b\": 382.53499560546874, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 33]}]'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='c0d54136-31de-42d4-9a33-7829ab385d4d', version=13, score=0.03630721, payload={'source_id': '980889bb-16cd-447f-b5eb-1384b84903cc', 'domain': 'finance', 'prov': '[{\"page_no\": 53, \"bbox\": {\"l\": 277.80267333984375, \"t\": 526.4105987548828, \"r\": 464.0212707519531, \"b\": 373.337646484375, \"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 0]}]'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Retrieved Result from docstore\n",
    "retrieved_doc_id = points[0].id\n",
    "\n",
    "retrieved_doc = doc_store.get(retrieved_doc_id)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "nodes = retrieved_doc.nodes\n",
    "print(len(nodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psiking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
